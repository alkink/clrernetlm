Transformer TabanlÄ± Åerit Tespiti YÃ¶ntemleri
Otonom sÃ¼rÃ¼ÅŸte ÅŸerit tespiti (lane detection), aracÄ±n ÅŸerit Ã§izgilerini algÄ±layarak konumunu gÃ¼venli biÃ§imde korumasÄ±nÄ± saÄŸlayan kritik bir gÃ¶revdir. Geleneksel derin Ã¶ÄŸrenme tabanlÄ± ÅŸerit tespiti genellikle konvolÃ¼syonel sinir aÄŸlarÄ± ve segmentasyon yaklaÅŸÄ±mÄ±na dayanÄ±rken, son dÃ¶nemde Transformer mimarileri ve dil-modeli yaklaÅŸÄ±mlarÄ± bu alana yenilikÃ§i Ã§Ã¶zÃ¼mler getirmiÅŸtir. AÅŸaÄŸÄ±da, transformer tabanlÄ± veya dil modeli temelli ÅŸerit tespiti yÃ¶ntemlerine odaklanan ve aÃ§Ä±k kaynak kod sunan Ã¶nemli akademik Ã§alÄ±ÅŸmalar Ã¶zetlenmiÅŸtir. (Not: LaneLM gibi yalnÄ±zca teori sunup kod paylaÅŸmayan Ã§alÄ±ÅŸmalar bu listeye dahil edilmemiÅŸtir.)
Ä°ki Boyutlu (2D) Åerit Tespitinde Transformer YaklaÅŸÄ±mlarÄ±
LaneATT (CVPR 2021)
Makale BaÅŸlÄ±ÄŸÄ±: Keep Your Eyes on the Lane: Real-time Attention-guided Lane Detection (Lucas Tabelini ve ark.)
openaccess.thecvf.com
YayÄ±n Tarihi: 2021 (CVPR 2021 konferansÄ±).
Mimari ve YÃ¶ntem: Anchor tabanlÄ± tek aÅŸamalÄ± bir derin ÅŸerit tespit modeli Ã¶nerilmiÅŸtir. Model, nesne tespitine benzer ÅŸekilde Ã¶nceden tanÄ±mlÄ± anchor bÃ¶lgelerinde Ã¶zellik havuzu (feature pooling) yapar ve anchor tabanlÄ± Ã¶zgÃ¼n bir dikkat mekanizmasÄ± (attention) ile kÃ¼resel sahne bilgisini toplar
openaccess.thecvf.com
openaccess.thecvf.com
. Hafif bir CNN omurgasÄ± kullanmasÄ±na raÄŸmen, anchor merkezli bu dikkat yaklaÅŸÄ±mÄ± sayesinde perdelenmiÅŸ veya eksik ÅŸerit Ã§izgilerini kÃ¼resel baÄŸlam yardÄ±mÄ±yla daha iyi tahmin edebilmektedir
openaccess.thecvf.com
.
AÃ§Ä±k Kaynak Kod: ResmÃ® aÃ§Ä±k kaynak kod ve Ã¶n eÄŸitimli modeller GitHub deposu Ã¼zerinden sunulmuÅŸtur (lucastabelini/LaneATT)
openaccess.thecvf.com
.
Benchmark SonuÃ§larÄ±: LaneATT modeli TuSimple, CULane ve LLAMAS gibi Ã¼Ã§ yaygÄ±n veri setinde kapsamlÄ± olarak deÄŸerlendirilmiÅŸ ve mevcut en iyi yÃ¶ntemleri hem doÄŸruluk hem de hÄ±z aÃ§Ä±sÄ±ndan geride bÄ±rakmÄ±ÅŸtÄ±r
openaccess.thecvf.com
. Ã–rneÄŸin CULane veri setinde hem daha yÃ¼ksek F1 skoru elde ederken, modelin gerÃ§ek zamanlÄ± Ã§alÄ±ÅŸacak kadar hÄ±zlÄ± olduÄŸu gÃ¶sterilmiÅŸtir (~250 FPS hÄ±z ile, Ã¶nceki en iyi modele kÄ±yasla hesaplama yÃ¼kÃ¼nÃ¼ neredeyse 10 kat azaltmÄ±ÅŸtÄ±r)
openaccess.thecvf.com
.
KatkÄ± ve Farklar: LaneATTâ€™nin en bÃ¼yÃ¼k yeniliÄŸi, ÅŸerit tespitine dikkat mekanizmasÄ±nÄ± entegre eden ilk anchor tabanlÄ± Ã§erÃ§eve olmasÄ±dÄ±r. Bu sayede model, gÃ¶rÃ¼ntÃ¼deki ÅŸeritlerin konumlarÄ±nÄ± diÄŸer ÅŸeritlerle iliÅŸkili ÅŸekilde kÃ¼resel Ã¶lÃ§ekte deÄŸerlendirerek Ã§Ä±karÄ±m yapar ve Ã¶zellikle Ã¶rtÃ¼lme (occlusion) veya silik ÅŸerit Ã§izgileri durumlarÄ±nda gÃ¼Ã§lÃ¼ bir performans sergiler
openaccess.thecvf.com
. AyrÄ±ca son derece hafif ve hÄ±zlÄ± olmasÄ±yla Ã¶ne Ã§Ä±kar: Ã–nceki Ã§alÄ±ÅŸmalarda gereken karmaÅŸÄ±k ardÄ±l iÅŸlemlere (Ã¶rn. yoÄŸun post-processing, NMS) ihtiyaÃ§ duymadan, gerÃ§ek zamanlÄ± Ã§alÄ±ÅŸÄ±rken o dÃ¶nemin en yÃ¼ksek doÄŸruluklarÄ±ndan birine ulaÅŸmÄ±ÅŸtÄ±r
openaccess.thecvf.com
. Bu yÃ¶nleriyle LaneATT, pratik otonom sÃ¼rÃ¼ÅŸ uygulamalarÄ± iÃ§in Ã¶nemli bir adÄ±m olmuÅŸtur.
LSTR â€“ Lane Shape Prediction with Transformers (WACV 2021)
Makale BaÅŸlÄ±ÄŸÄ±: End-to-End Lane Shape Prediction with Transformers (Ruijin Liu ve ark.)
ar5iv.labs.arxiv.org
YayÄ±n Tarihi: 2021 (WACV 2021 konferansÄ±).
Mimari ve YÃ¶ntem: LSTR, ÅŸerit tespit problemini bir dil modeli gibi dizisel bir Ã§Ä±ktÄ± yerine doÄŸrudan parametrik bir eÄŸri olarak Ã§Ã¶zen ilk yÃ¶ntemlerdendir. Tamamen transformer tabanlÄ± bir uÃ§tan uca aÄŸ kullanarak, gÃ¶rÃ¼ntÃ¼den her bir ÅŸeridin polinom eÄŸri parametrelerini doÄŸrudan tahmin eder
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Bu mimari, kÃ¼resel baÄŸlam ve ÅŸeritlerin uzun-ince yapÄ±sÄ±nÄ± yakalamak iÃ§in kendine dikkat mekanizmalarÄ±ndan (self-attention) yararlanÄ±r. AÄŸÄ±n Ã§Ä±ktÄ±larÄ±, her bir ÅŸeride ait parametre gruplarÄ±dÄ±r ve klasik yÃ¶ntemlerin aksine yoÄŸun bir piksel segmentasyonu yerine bu parametrelerin Ã¶ÄŸrenilmesini hedefler
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
.
AÃ§Ä±k Kaynak Kod: ResmÃ® kod deposu mevcuttur (liuruijin17/LSTR) ve deneylerin tekrarlanabilmesi iÃ§in paylaÅŸÄ±lmÄ±ÅŸtÄ±r
ar5iv.labs.arxiv.org
.
Benchmark SonuÃ§larÄ±: LSTR modeli, TuSimple ÅŸerit tespiti benchmarkâ€™Ä±nda %96.18 doÄŸruluk gibi son derece yÃ¼ksek bir skora ulaÅŸarak o dÃ¶nemki duruma gÃ¶re yeni bir seviye belirlemiÅŸtir
github.com
. Model sadece baÅŸarÄ±lÄ± deÄŸil, aynÄ± zamanda son derece hafiftir: toplam parametre sayÄ±sÄ± 0.76M mertebesinde ve hesaplama yÃ¼kÃ¼ 574 milyon MAC civarÄ±ndadÄ±r, bu da gerÃ§ek zamanlÄ± uygulamalarda bÃ¼yÃ¼k avantaj saÄŸlar
github.com
github.com
. En yakÄ±n rakiplerine benzer veya daha yÃ¼ksek doÄŸruluÄŸa ulaÅŸÄ±rken model boyutu ve hÄ±z aÃ§Ä±sÄ±ndan en iyilerden biri olduÄŸunu kanÄ±tlamÄ±ÅŸtÄ±r.
KatkÄ± ve Farklar: Bu Ã§alÄ±ÅŸmanÄ±n yenilikÃ§i katkÄ±sÄ±, ÅŸerit tespitini tek adÄ±mlÄ± bir regresyon problemi olarak ele almasÄ±dÄ±r. LSTR, Ã§Ä±ktÄ± olarak piksellerden oluÅŸan bir maskeyi deÄŸil, her bir ÅŸeridi tanÄ±mlayan matematiksel parametreleri verir. Bunu mÃ¼mkÃ¼n kÄ±lmak iÃ§in literatÃ¼rde ilk defa transfomer bloÄŸunu kullanarak ÅŸerit noktalarÄ± arasÄ±ndaki uzun menzilli iliÅŸkileri ve global sahne bilgisini Ã¶ÄŸrenmiÅŸtir
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Model, Ã§Ä±ktÄ± ÅŸerit parametrelerini gerÃ§ek ÅŸeritlerle eÅŸleÅŸtirmek iÃ§in Hungarian eÅŸleÅŸtirme tabanlÄ± bir kayÄ±p fonksiyonu kullanÄ±r ve bu sayede bir gÃ¶rÃ¼ntÃ¼deki her ÅŸeridi tekil bir hedef olarak Ã¶ÄŸrenir
ar5iv.labs.arxiv.org
. Bu tasarÄ±mÄ±n Ã¶nemli bir avantajÄ±, son iÅŸlem olarak Non-Maximum Suppression (NMS) gereksinimini ortadan kaldÄ±rmasÄ± ve daha basit bir Ã§Ä±karÄ±m sÃ¼reci sunmasÄ±dÄ±r
ar5iv.labs.arxiv.org
. SonuÃ§ olarak LSTR, hem yÃ¼ksek doÄŸruluk hem de en hÄ±zlÄ± inference Ã¶zelliklerini bir araya getirerek, ÅŸerit tespitinde transformer kullanÄ±mÄ±nÄ±n etkinliÄŸini gÃ¶stermiÅŸtir.
LaneFormer (AAAI 2022)
Makale BaÅŸlÄ±ÄŸÄ±: Laneformer: Object-Aware Row-Column Transformers for Lane Detection (Jianhua Han ve ark.)
arxiv.org
YayÄ±n Tarihi: 2022 (AAAI 2022 konferansÄ±).
Mimari ve YÃ¶ntem: LaneFormer, ÅŸerit algÄ±lama iÃ§in Ã¶zelleÅŸtirilmiÅŸ bir encoder-decoder Transformer mimarisi sunar. BaÅŸlÄ±ca yenilik, satÄ±r ve sÃ¼tun boyutlu kendiliÄŸinden dikkat mekanizmalarÄ±nÄ±n getirilmesidir: Encoder aÅŸamasÄ±nda, her bir piksel Ã¶zelliÄŸi Ã¼zerinde â€œrow-attentionâ€ (aynÄ± yatay satÄ±rdaki pikseller arasÄ± etkileÅŸim) ve â€œcolumn-attentionâ€ (aynÄ± dikey sÃ¼tundaki pikseller arasÄ± etkileÅŸim) iÅŸlemleri uygulanÄ±r
cdn.aaai.org
. Bu sayede model, ÅŸerit Ã§izgilerinin gÃ¶rÃ¼ntÃ¼deki geometrik ÅŸekillerini daha etkin yakalar: ArdÄ±ÅŸÄ±k satÄ±rlarda aynÄ± ÅŸeride ait piksellerin yakÄ±n konumda olacaÄŸÄ± bilgisi satÄ±r-dikkat ile iÅŸlenirken; farklÄ± ÅŸeritlerin ayrÄ± sÃ¼tunlarda bulunmasÄ± durumu sÃ¼tun-dikkat ile ayrÄ±ÅŸtÄ±rÄ±lÄ±r (Åekil 1â€™de ÅŸematize edilmiÅŸtir)
cdn.aaai.org
. AyrÄ±ca LaneFormer, ortamdaki nesne algÄ±lamalarÄ±nÄ± da Transformerâ€™e entegre eden bir yaklaÅŸÄ±ma sahiptir. Ã–ncÃ¼l bir nesne tespit modeliyle bulunan araÃ§/yaya gibi nesnelerin bulanÄ±k kutu konumlarÄ± (bbox), transformerâ€™Ä±n dikkat mekanizmasÄ±nda Key olarak, bu kutulardan Ã§Ä±karÄ±lan ROI Ã¶zellik vektÃ¶rleri ise Value olarak beslenir
arxiv.org
. BÃ¶ylece ÅŸerit tespiti sÄ±rasÄ±nda model, etraftaki nesnelerin varlÄ±ÄŸÄ±nÄ± ve konumunu da gÃ¶z Ã¶nÃ¼ne alarak (Ã¶r. araÃ§larÄ±n hemen yanÄ±nda ÅŸerit olma olasÄ±lÄ±ÄŸÄ± gibi) daha baÄŸlamsal bir Ã§Ä±karÄ±m yapar
arxiv.org
.
AÃ§Ä±k Kaynak Kod: ResmÃ® kod paylaÅŸÄ±lmÄ±ÅŸtÄ±r (Huawei Noahâ€™s Ark Lab â€“ Codes for Lane Detection reposu altÄ±nda) ve deneylerin tekrarÄ± iÃ§in eriÅŸilebilir durumdadÄ±r
researchgate.net
.
Benchmark SonuÃ§larÄ±: LaneFormer, CULane veri setinde %77.1 F1 skoru elde ederek o zamana kadarki en yÃ¼ksek performansÄ± sergilemiÅŸtir
cdn.aaai.org
. AynÄ± model, TuSimple veri setinde de %96.8 doÄŸruluk baÅŸarÄ±mÄ±na ulaÅŸmÄ±ÅŸ, bÃ¶ylece 2D ÅŸerit tespitinde hem ÅŸehir iÃ§i karmaÅŸÄ±k sahnelerde (CULane) hem de otoyol senaryolarÄ±nda (TuSimple) Ã¼stÃ¼nlÃ¼ÄŸÃ¼nÃ¼ kanÄ±tlamÄ±ÅŸtÄ±r
cdn.aaai.org
. Modelin verimliliÄŸi de yÃ¼ksektir; ResNet-50 tabanlÄ± LaneFormer, tek bir GPU Ã¼zerinde ~50 FPS hÄ±zÄ±na varan gerÃ§ek zamanlÄ± performans raporlamÄ±ÅŸtÄ±r
cdn.aaai.org
.
KatkÄ± ve Farklar: LaneFormer, ÅŸerit algÄ±lama problemine uzamsal dikkat (spatial attention) konusunda yeni bir bakÄ±ÅŸ aÃ§Ä±sÄ± getirmiÅŸtir. Ã–zellikle, ilk defa satÄ±r ve sÃ¼tun yÃ¶nelimli dikkat bileÅŸenleri kullanÄ±larak ÅŸeritlerin uzunlamasÄ±na yapÄ±sÄ± ve paralelliÄŸi etkin ÅŸekilde modele dahil edilmiÅŸtir. Bu yapÄ±, global-baÄŸlamsal Ã¶ÄŸrenme ile yerel geometrik kÄ±sÄ±tlarÄ± birleÅŸtirerek, NMS veya kÃ¼melenmiÅŸ post-processing adÄ±mlarÄ±na ihtiyaÃ§ duymadan doÄŸrudan doÄŸruya ÅŸerit tespiti yapabilmeyi saÄŸlar
cdn.aaai.org
ar5iv.labs.arxiv.org
. DahasÄ±, nesne farkÄ±ndalÄ±ÄŸÄ±nÄ±n dikkat mekanizmasÄ±na entegre edilmesi, otonom sÃ¼rÃ¼ÅŸ sahnelerinde sÄ±kÃ§a rastlanan araÃ§ trafiÄŸi ve engellemeler altÄ±nda modelin dayanÄ±klÄ±lÄ±ÄŸÄ±nÄ± artÄ±rmÄ±ÅŸtÄ±r. LaneFormer ile gÃ¶sterilen bir diÄŸer Ã¶nemli Ã§Ä±ktÄ±, transformer tabanlÄ± bir modelin optimize implementasyon ile gerÃ§ek zamanlÄ± hÄ±zlara yakÄ±n Ã§alÄ±ÅŸabileceÄŸidir (yaklaÅŸÄ±k 48â€“53 FPS)
cdn.aaai.org
. Bu yÃ¶nÃ¼yle LaneFormer, hem akademik hem endÃ¼striyel aÃ§Ä±dan 2D ÅŸerit tespitinde gÃ¼Ã§lÃ¼ bir temel oluÅŸturmuÅŸtur.
CondLane (CondLSTR, ICCV 2023)
Makale BaÅŸlÄ±ÄŸÄ±: Generating Dynamic Kernels via Transformers for Lane Detection (Ziye Chen ve ark.)
openaccess.thecvf.com
openaccess.thecvf.com
YayÄ±n Tarihi: 2023 (ICCV 2023 konferansÄ±).
Mimari ve YÃ¶ntem: CondLane (makalede CondLSTR olarak da anÄ±lÄ±r), ÅŸerit tespiti iÃ§in dinamik evriÅŸim Ã§ekirdekleri Ã¼reten bir transformer mimarisi sunar. Bu yÃ¶ntemde bir transformer bloÄŸu, gÃ¶rÃ¼ntÃ¼deki her bir ÅŸerit Ã§izgisi iÃ§in Ã¶zel bir konvolÃ¼syon Ã§ekirdeÄŸini dinamik olarak oluÅŸturur; ardÄ±ndan bu Ã§ekirdek, Ã¶zellik haritasÄ± Ã¼zerinde katmanlÄ± evriÅŸim ÅŸeklinde uygulananarak ilgili ÅŸeridi tespit eder
openaccess.thecvf.com
. Klasik yaklaÅŸÄ±mda dinamik Ã§ekirdekler genellikle sadece ÅŸerit baÅŸlangÄ±Ã§ noktasÄ± gibi sÄ±nÄ±rlÄ± bir bÃ¶lgeden tÃ¼retilirken, CondLaneâ€™de transformer global ÅŸerit bilgisini Ã¶ÄŸrenerek Ã§ekirdekleri Ã¼retir. Bu sayede Ã¼retilen filtreler, ÅŸeridin tÃ¼m eÄŸrisel yapÄ±sÄ±nÄ± (uzun ve kÄ±vrÄ±mlÄ± olsa dahi) hesaba katar ve ÅŸeritlerin Ã§atallanmasÄ±, kesiÅŸmesi veya araÃ§larla Ã¶rtÃ¼lmesi durumlarÄ±nda bile saÄŸlam bir tespit gerÃ§ekleÅŸtirir
openaccess.thecvf.com
openaccess.thecvf.com
. BaÅŸka bir deyiÅŸle, model belirli bir geometrik form varsayÄ±mÄ±na (Ã¶rneÄŸin yalnÄ±zca dÃ¼z Ã§izgi ya da polinom) dayanmaz; bunun yerine veriden Ã¶ÄŸrenilen esnek Ã§ekirdeklerle her tÃ¼rlÃ¼ ÅŸerit topolojisine uyum saÄŸlar
openaccess.thecvf.com
openaccess.thecvf.com
.
AÃ§Ä±k Kaynak Kod: Bu Ã§alÄ±ÅŸmanÄ±n PyTorch tabanlÄ± resmi uygulamasÄ± GitHub Ã¼zerinde paylaÅŸÄ±lmÄ±ÅŸtÄ±r (czyczyyzc/CondLSTR deposu) ve araÅŸtÄ±rmacÄ±larÄ±n kullanÄ±mÄ±na sunulmuÅŸtur. README dokÃ¼manÄ±nda yÃ¶ntemin Ã§erÃ§evesi ve kullanÄ±m adÄ±mlarÄ± ayrÄ±ntÄ±lÄ± olarak verilmiÅŸtir
github.com
.
Benchmark SonuÃ§larÄ±: CondLane yÃ¶ntemi, mevcut en iyi yÃ¶ntemleri Ã¶nemli farklarla geride bÄ±rakarak yeni rekorlar kÄ±rmÄ±ÅŸtÄ±r. Ã–rneÄŸin OpenLane (3D ÅŸeritler iÃ§eren aÃ§Ä±k ortam veri seti) Ã¼zerinde F1 skoru 63.40 elde ederek Ã¶nceki en iyi sonucu +4.30 puan geliÅŸtirmiÅŸtir. Benzer ÅŸekilde CurveLanes (zorlayÄ±cÄ± eÄŸri ÅŸerit veri seti) Ã¼zerinde F1 skoru 88.47 ile bir Ã¶nceki en iyi yÃ¶ntemi +2.37 puan aÅŸmÄ±ÅŸtÄ±r
openaccess.thecvf.com
. Bu belirgin iyileÅŸtirmeler, Ã¶zellikle karmaÅŸÄ±k yapÄ±lÄ± ve dÃ¶nÃ¼ÅŸlÃ¼ ÅŸeritlerin olduÄŸu senaryolarda CondLaneâ€™in Ã¼stÃ¼n performans gÃ¶sterdiÄŸine iÅŸaret etmektedir.
KatkÄ± ve Farklar: CondLane, ÅŸerit tespitinde Ã¶zel bilgiye dayalÄ± modellerden (Ã¶r. belirli polinom/spline varsayÄ±mlarÄ±) genel Ã¶ÄŸrenilebilir modellere geÃ§iÅŸi simgeleyen bir Ã§alÄ±ÅŸmadÄ±r. Transformer tabanlÄ± dinamik Ã§ekirdek Ã¼retimi sayesinde, model ÅŸerit ÅŸekillerinin global yapÄ±sÄ±nÄ± doÄŸrudan Ã¶ÄŸrenip evriÅŸim filtrelerine yansÄ±tÄ±r. Bu yaklaÅŸÄ±m, Ã¶zellikle Ã§atal yapan veya birden fazla kola ayrÄ±lan ÅŸeritler, yoÄŸun biÃ§imde paralel giden ÅŸeritler ve araÃ§larca kÄ±smen gizlenmiÅŸ ÅŸeritler gibi durumlarda, Ã¶nceki sabit Ã§ekirdek kullanan yÃ¶ntemlere kÄ±yasla belirgin bir avantaj saÄŸlar
openaccess.thecvf.com
openaccess.thecvf.com
. CondLaneâ€™in bir diÄŸer farkÄ±, ÅŸerit tespit sÃ¼recini transformer ile Ã¶ÄŸrenilebilir bir alt aÄŸa dÃ¶nÃ¼ÅŸtÃ¼rmesidir: Bu sayede model, sahne iÃ§indeki her ÅŸeridi bir sÄ±ra dizisi ya da maskeden ziyade, o ÅŸeride Ã¶zgÃ¼ bir filtre olarak temsil eder. Bu yenilikÃ§i bakÄ±ÅŸ aÃ§Ä±sÄ±, dil modellerindeki sÄ±ra-tabanlÄ± yaklaÅŸÄ±m ile gÃ¶rÃ¼ntÃ¼ tabanlÄ± evriÅŸimsel yaklaÅŸÄ±mlar arasÄ±nda bir kÃ¶prÃ¼ kurarak literatÃ¼re katkÄ± sunmuÅŸtur. SonuÃ§ olarak CondLane, 2B ÅŸerit tespitinde hem esneklik hem de doÄŸruluk aÃ§Ä±sÄ±ndan Ã¶nemli bir ilerlemeyi temsil etmektedir.
ÃœÃ§ Boyutlu (3D) Åerit Tespitinde Transformer YaklaÅŸÄ±mlarÄ±
PersFormer (ECCV 2022)
Makale BaÅŸlÄ±ÄŸÄ±: PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark (Li Chen ve ark.)
ar5iv.labs.arxiv.org
YayÄ±n Tarihi: 2022 (ECCV 2022, sÃ¶zlÃ¼ sunum).
Mimari ve YÃ¶ntem: PersFormer, monokÃ¼ler kamera gÃ¶rÃ¼ntÃ¼sÃ¼nden 3B ÅŸeritleri tespit eden uÃ§tan uca bir modeldir. Ana yeniliÄŸi, perspektiften kuÅŸbakÄ±ÅŸÄ±na (BEV) Ã¶zellik dÃ¶nÃ¼ÅŸÃ¼mÃ¼nÃ¼ bir Transformer modÃ¼lÃ¼ ile gerÃ§ekleÅŸtirmesidir
ar5iv.labs.arxiv.org
. Modelin â€œperspective transformerâ€ adÄ± verilen bileÅŸeni, kamera iÃ§ ve dÄ±ÅŸ parametrelerini referans alarak Ã¶n-iz gÃ¶rÃ¼ntÃ¼deki lokal bÃ¶lgeleri kuÅŸbakÄ±ÅŸÄ± dÃ¼zleme aktarÄ±r; bunu yaparken Ã§oklu baÅŸlÄ±kli dikkat mekanizmasÄ± kullanarak, her BEV konumunun ilgili olduÄŸu Ã¶n-iz bÃ¶lgesini Ã¶ÄŸrenir
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Bu sayede geleneksel sabit dÃ¶nÃ¼ÅŸtÃ¼rme (Ã¶r. Inverse Perspective Mapping) yÃ¶ntemlerinde yaÅŸanan derinlik hizalama hatalarÄ± en aza indirilir. PersFormer, aynÄ± zamanda birleÅŸik bir 2B/3B anchor tasarÄ±mÄ± kullanÄ±r ve 2B ile 3B ÅŸerit tespitini eÅŸ-zamanlÄ± yapan Ã§ok-gÃ¶revli bir aÄŸdÄ±r; ortak bir Ã¶ÄŸrenme ile 2B ve 3B Ã§Ä±ktÄ±lar birbirini destekleyerek daha tutarlÄ± Ã¶zellikler elde edilmesini saÄŸlar
ar5iv.labs.arxiv.org
github.com
. Ã–zetle, kamera gÃ¶rÃ¼ntÃ¼sÃ¼nden doÄŸrudan 3B uzayda ÅŸeritleri Ã§Ä±karabilmek iÃ§in hem geometrik dÃ¶nÃ¼ÅŸÃ¼mÃ¼ Ã¶ÄŸrenen, hem de 3B verisizlik problemini (yÃ¼ksekliÄŸin belirsizliÄŸi) kameranÄ±n zemin dÃ¼zlemine gÃ¶re Ã§Ã¶zen bir mimari sunulmuÅŸtur.
AÃ§Ä±k Kaynak Kod: Projenin kodlarÄ± ve eÄŸitimli model aÄŸÄ±rlÄ±klarÄ± aÃ§Ä±k kaynak olarak paylaÅŸÄ±lmÄ±ÅŸtÄ±r (OpenDriveLab/PersFormer_3DLane GitHub deposu)
ar5iv.labs.arxiv.org
. AyrÄ±ca Ã§alÄ±ÅŸma kapsamÄ±nda otonom sÃ¼rÃ¼ÅŸ iÃ§in kapsamlÄ± bir 3B ÅŸerit veri seti olan OpenLane de yayÄ±nlanmÄ±ÅŸtÄ±r (200k gÃ¶rÃ¼ntÃ¼ karesi, ~880k ÅŸerit Ã¶rneÄŸi iÃ§erir)
ar5iv.labs.arxiv.org
.
Benchmark SonuÃ§larÄ±: PersFormer, sunulan OpenLane veri setinde ve Apollo 3D SÃ¼rÃ¼m (simÃ¼lasyon) veri setinde mevcut yÃ¶ntemleri kayda deÄŸer farkla geride bÄ±rakmÄ±ÅŸtÄ±r
ar5iv.labs.arxiv.org
. Ã–rneÄŸin, OpenLane Ã¼zerinde F1 skoru ~53 civarÄ±nda elde edilerek, bir Ã¶nceki en iyi yÃ¶ntem olan 3D-LaneNetâ€™in (~44 F1) oldukÃ§a Ã¼zerine Ã§Ä±kÄ±lmÄ±ÅŸtÄ±r
github.com
. Apollo simÃ¼lasyon ortamÄ±nda ve ONCE-3DLanes gibi diÄŸer benchmarkâ€™larda da benzer Ã¼stÃ¼nlÃ¼kler raporlanmÄ±ÅŸtÄ±r. AyrÄ±ca PersFormer, OpenLane veri setinin 2B ÅŸerit tespiti kÄ±smÄ±nda da Ã§aÄŸdaÅŸ 2B yÃ¶ntemlerle kÄ±yaslanabilir bir doÄŸruluk yakalayarak Ã§ok yÃ¶nlÃ¼lÃ¼ÄŸÃ¼nÃ¼ gÃ¶stermiÅŸtir
ar5iv.labs.arxiv.org
.
KatkÄ± ve Farklar: PersFormerâ€™in en Ã¶nemli katkÄ±sÄ±, gÃ¶rÃ¼ntÃ¼ uzayÄ±ndan BEV uzayÄ±na Ã¶ÄŸrenilebilir bir dÃ¶nÃ¼ÅŸÃ¼m gerÃ§ekleÅŸtirerek 3B ÅŸerit tespitindeki temel sorunlardan birini Ã§Ã¶zmesidir. Klasik yÃ¶ntemlerdeki plan projeksiyon varsayÄ±mÄ±nÄ±n Ã¶tesine geÃ§erek, kamera gÃ¶rÃ¼ÅŸ aÃ§Ä±sÄ± deÄŸiÅŸimleri, yol eÄŸimleri gibi durumlarda dahi gÃ¼venilir ÅŸerit Ã§Ä±karÄ±mÄ± mÃ¼mkÃ¼n kÄ±lÄ±nmÄ±ÅŸtÄ±r
ar5iv.labs.arxiv.org
. AyrÄ±ca bu Ã§alÄ±ÅŸma ile birlikte yayÄ±nlanan OpenLane veri seti, gerÃ§ek dÃ¼nyadan yÃ¼ksek hacimli 3B ÅŸerit verisi sunarak alanda bir standart oluÅŸturmuÅŸtur
ar5iv.labs.arxiv.org
. PersFormer modeli, 3B ÅŸerit algÄ±lama iÃ§in o gÃ¼ne kadarki en iyi sonuÃ§larÄ± saÄŸlamakla kalmayÄ±p, aynÄ± aÄŸ iÃ§inde 2B ve 3B algÄ±lama yaparak Ã§ok-gÃ¶revli Ã¶ÄŸrenmenin faydalarÄ±nÄ± ortaya koymuÅŸtur
ar5iv.labs.arxiv.org
ar5iv.labs.arxiv.org
. Bu yÃ¶nÃ¼yle PersFormer, otonom sÃ¼rÃ¼ÅŸ algÄ± sistemlerinde kameradan 3B anlayÄ±ÅŸ Ã§Ä±karma problemini pratikÃ§e Ã§Ã¶zen Ã¶ncÃ¼ bir Ã§alÄ±ÅŸma olarak deÄŸerlendirilebilir.
LATR (ICCV 2023)
Makale BaÅŸlÄ±ÄŸÄ±: LATR: 3D Lane Detection from Monocular Images with Transformer (Yueru Luo ve ark.)
arxiv.org
YayÄ±n Tarihi: 2023 (ICCV 2023, sÃ¶zlÃ¼ sunum).
Mimari ve YÃ¶ntem: LATR, monokÃ¼ler bir gÃ¶rÃ¼ntÃ¼den 3B ÅŸeritleri tespit etmek iÃ§in tasarlanmÄ±ÅŸ bir transformer tabanlÄ± algÄ±layÄ±cÄ±dÄ±r. Bu model, 3B uzaysal bilgisini kullanmak iÃ§in aÃ§Ä±kÃ§a bir BEV gÃ¶rÃ¼ntÃ¼ oluÅŸturmaya gerek duymadan, gÃ¶rÃ¼ntÃ¼ Ã¶zelliklerini 3B farkÄ±ndalÄ±klÄ± hale getirmektedir
arxiv.org
. LATR mimarisinde, gÃ¶rÃ¼ntÃ¼den Ã§Ä±karÄ±lan Ã¶zellikler iÃ§erisine, iteratif olarak gÃ¼ncellenen bir sanal 3B zemin dÃ¼zlemine gÃ¶re hesaplanan konumsal gÃ¶mme vektÃ¶rleri eklenir
arxiv.org
. ArdÄ±ndan model, belirli sayÄ±da Ã¶ÄŸrenilebilir sorgu (query) tanÄ±mlayarak, sorgu-ve-anahtar-deÄŸer tabanlÄ± bir Ã§apraz-dikkat mekanizmasÄ± kurar
arxiv.org
. Her bir sorgu vektÃ¶rÃ¼, 2B gÃ¶rÃ¼ntÃ¼deki olasÄ± bir ÅŸerit adayÄ±nÄ±n Ã¶zelliklerinden Ã¼retilir ve bu sorgular, gÃ¶rÃ¼ntÃ¼ Ã¶zellik haritasÄ±ndaki anahtar-deÄŸer ikilileriyle etkileÅŸime girerek doÄŸrudan ilgili ÅŸeridin 3B koordinatlarÄ±nÄ± tahmin eder
arxiv.org
. Bu yaklaÅŸÄ±m, Ã¶zellikle monokÃ¼ler gÃ¶rÃ¼ntÃ¼lerde yaÅŸanan derinlik belirsizliÄŸi sorununu azaltmayÄ± hedefler; zira LATR, perspektif gÃ¶rÃ¼ntÃ¼deki Ã¶zellikleri doÄŸrudan 3B dÃ¼nyadaki zemine gÃ¶re konumlandÄ±rarak, kuÅŸbakÄ±ÅŸÄ± dÃ¶nÃ¼ÅŸÃ¼m olmaksÄ±zÄ±n ÅŸeritleri konumlandÄ±rabilir.
AÃ§Ä±k Kaynak Kod: Bu Ã§alÄ±ÅŸmanÄ±n resmi kod deposu (JMoonr/LATR) mevcuttur ve ICCV 2023 itibariyle araÅŸtÄ±rmacÄ±larla paylaÅŸÄ±lmÄ±ÅŸtÄ±r
github.com
. Kod ile birlikte model aÄŸÄ±rlÄ±klarÄ± ve kullanÄ±m Ã¶rnekleri de sunulmaktadÄ±r.
Benchmark SonuÃ§larÄ±: LATR, 3B ÅŸerit tespiti alanÄ±nda yayÄ±nlandÄ±ÄŸÄ± dÃ¶nemde yeni standartlar belirlemiÅŸtir. Apollo (sentetik) veri seti, OpenLane (gerÃ§ek dÃ¼nya) ve ONCE-3DLanes gibi Ã§eÅŸitli benchmarkâ€™larda Ã¶nceki en iyi sonuÃ§larÄ± ciddi farklarla aÅŸmÄ±ÅŸtÄ±r
arxiv.org
. Ã–rneÄŸin OpenLane veri setinde LATR, F1 skorunu Ã¶nceki en iyinin tam +11.4 puan Ã¼zerine Ã§Ä±kararak bÃ¼yÃ¼k bir performans artÄ±ÅŸÄ± saÄŸlamÄ±ÅŸtÄ±r (Ã¶nceki ~%42 F1â€™den LATR ile ~%53.4 F1â€™e)
arxiv.org
. Bu kazanÄ±mlar, LATRâ€™nin Ã¶zellikle karmaÅŸÄ±k sahnelerde (monokÃ¼ler kameranÄ±n dezavantajlÄ± olduÄŸu derinlik belirsizliklerinde) bile Ã¼stÃ¼n bir kesinlikte ÅŸerit tespiti yapabildiÄŸini gÃ¶stermektedir.
KatkÄ± ve Farklar: LATRâ€™nin geliÅŸtirilmesiyle monokÃ¼ler kamera gÃ¶rÃ¼ntÃ¼lerinden 3B ÅŸerit tespitine yÃ¶nelik yeni bir paradigma ortaya konmuÅŸtur. PersFormer gibi yÃ¶ntemler BEV dÃ¶nÃ¼ÅŸÃ¼mÃ¼ kullanÄ±rken, LATR bunun yerine 3B bilgisini doÄŸrudan dikkat mekanizmasÄ±na entegre ederek perspektif kayÄ±plarÄ± ve hizalama hatalarÄ±nÄ± minimize etmiÅŸtir
arxiv.org
. Modelde tanÄ±mlanan Ã¶ÄŸrenilebilir sorgular ve dinamik 3B pozisyonel gÃ¶mme tekniÄŸi, her bir ÅŸeridin 3B uzaydaki ÅŸeklini kademeli olarak iyileÅŸtirerek tahmin etme imkÃ¢nÄ± tanÄ±r
arxiv.org
. Bu yenilikler sayesinde LATR, 3B ÅŸerit algÄ±lama performansÄ±nda Ã§arpÄ±cÄ± bir sÄ±Ã§rama elde etmiÅŸ ve derin Ã¶ÄŸrenme modellerinin 3B uzayda baÄŸlamsal bilgi kullanÄ±mÄ± konusunda etkileyici bir Ã¶rnek teÅŸkil etmiÅŸtir (OpenLane benchmarkâ€™Ä±ndaki +11.4 puanlÄ±k F1 artÄ±ÅŸÄ± buna somut bir kanÄ±ttÄ±r)
arxiv.org
.
Kaynaklar: YukarÄ±da anÄ±lan Ã§alÄ±ÅŸmalarÄ±n bilgileri ilgili makalelerin Ã¶zetlerinden ve raporladÄ±klarÄ± deneysel sonuÃ§lardan derlenmiÅŸtir. Her bir baÅŸlÄ±k altÄ±nda verilen referanslar (ã€xâ€ Ly-Lzã€‘ biÃ§iminde) doÄŸrudan ilgili makaleye veya proje sayfasÄ±na ait olup, daha fazla detay iÃ§in incelenmeleri Ã¶nerilir. Bu derlemede yalnÄ±zca aÃ§Ä±k kaynak kodu saÄŸlanmÄ±ÅŸ ve deneysel olarak doÄŸrulanmÄ±ÅŸ yÃ¶ntemler ele alÄ±nmÄ±ÅŸtÄ±r.
AlÄ±ntÄ±lar

Keep Your Eyes on the Lane: Real-Time Attention-Guided Lane Detection

https://openaccess.thecvf.com/content/CVPR2021/papers/Tabelini_Keep_Your_Eyes_on_the_Lane_Real-Time_Attention-Guided_Lane_Detection_CVPR_2021_paper.pdf

Keep Your Eyes on the Lane: Real-Time Attention-Guided Lane Detection

https://openaccess.thecvf.com/content/CVPR2021/papers/Tabelini_Keep_Your_Eyes_on_the_Lane_Real-Time_Attention-Guided_Lane_Detection_CVPR_2021_paper.pdf

Keep Your Eyes on the Lane: Real-Time Attention-Guided Lane Detection

https://openaccess.thecvf.com/content/CVPR2021/papers/Tabelini_Keep_Your_Eyes_on_the_Lane_Real-Time_Attention-Guided_Lane_Detection_CVPR_2021_paper.pdf

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

GitHub - liuruijin17/LSTR: This is an official repository of End-to-end Lane Shape Prediction with Transformers.

https://github.com/liuruijin17/LSTR

GitHub - liuruijin17/LSTR: This is an official repository of End-to-end Lane Shape Prediction with Transformers.

https://github.com/liuruijin17/LSTR

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2203.09830] Laneformer: Object-aware Row-Column Transformers for Lane Detection

https://arxiv.org/abs/2203.09830
Laneformer: Object-Aware Row-Column Transformers for Lane Detection

https://cdn.aaai.org/ojs/19961/19961-13-23974-1-2-20220628.pdf

[2203.09830] Laneformer: Object-aware Row-Column Transformers for Lane Detection

https://arxiv.org/abs/2203.09830

(PDF) Laneformer: Object-aware Row-Column Transformers for ...

https://www.researchgate.net/publication/359367973_Laneformer_Object-aware_Row-Column_Transformers_for_Lane_Detection
Laneformer: Object-Aware Row-Column Transformers for Lane Detection

https://cdn.aaai.org/ojs/19961/19961-13-23974-1-2-20220628.pdf
Laneformer: Object-Aware Row-Column Transformers for Lane Detection

https://cdn.aaai.org/ojs/19961/19961-13-23974-1-2-20220628.pdf
Laneformer: Object-Aware Row-Column Transformers for Lane Detection

https://cdn.aaai.org/ojs/19961/19961-13-23974-1-2-20220628.pdf

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

GitHub - czyczyyzc/CondLSTR: Code for paper "Generating Dynamic Kernels via Transformers for Lane Detection"

https://github.com/czyczyyzc/CondLSTR

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

Generating Dynamic Kernels via Transformers for Lane Detection

https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_Generating_Dynamic_Kernels_via_Transformers_for_Lane_Detection_ICCV_2023_paper.pdf

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

GitHub - OpenDriveLab/PersFormer_3DLane: [ECCV 2022 Oral] Perspective Transformer on 3D Lane Detection

https://github.com/OpenDriveLab/PersFormer_3DLane

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

GitHub - OpenDriveLab/PersFormer_3DLane: [ECCV 2022 Oral] Perspective Transformer on 3D Lane Detection

https://github.com/OpenDriveLab/PersFormer_3DLane

[2203.11089] PersFormer: 3D Lane Detection via Perspective Transformer and the OpenLane Benchmark

https://ar5iv.labs.arxiv.org/html/2203.11089

[2308.04583] LATR: 3D Lane Detection from Monocular Images with Transformer

https://arxiv.org/abs/2308.04583

[2308.04583] LATR: 3D Lane Detection from Monocular Images with Transformer

https://arxiv.org/abs/2308.04583

[2308.04583] LATR: 3D Lane Detection from Monocular Images with Transformer

https://arxiv.org/abs/2308.04583

GitHub - JMoonr/LATR: [ICCV2023 Oral] LATR: 3D Lane Detection from Monocular Images with Transformer

https://github.com/JMoonr/LATR

[2308.04583] LATR: 3D Lane Detection from Monocular Images with Transformer

https://arxiv.org/abs/2308.04583

[2308.04583] LATR: 3D Lane Detection from Monocular Images with Transformer

https://arxiv.org/abs/2308.04583

Makaleyi incelediÄŸimde, LaneLM isimli bu Ã§alÄ±ÅŸma aÃ§Ä±kÃ§a Transformer mimarisine dayalÄ± bir yaklaÅŸÄ±mdÄ±r ve Ã¶zellikle dil modeli benzeri bir yapÄ± ile lane detection (ÅŸerit tespiti) gÃ¶revini Ã§Ã¶zmeyi hedeflemektedir. AÅŸaÄŸÄ±da bunu adÄ±m adÄ±m, sade bir dille aÃ§Ä±klayayÄ±m:

ğŸŒ Bu Makale Ne YapÄ±yor?

LaneLM, ÅŸerit tespiti problemini klasik â€œgÃ¶rÃ¼ntÃ¼den ÅŸekil bulmaâ€ yerine bir dil anlama ve Ã¼retme problemi gibi ele alÄ±yor. Yani:

Her bir ÅŸerit Ã§izgisi, bir kelime dizisi (token sequence) gibi temsil ediliyor.

Model, gÃ¶rÃ¼ntÃ¼deki ÅŸeritleri, tÄ±pkÄ± bir dil modeli gibi, sÄ±rayla tahmin ediyor (Ã¶rneÄŸin: bir kelimeden sonra hangisi gelir? â†’ bir noktadan sonra ÅŸerit nereden geÃ§er?).

Bu sÃ¼reci yÃ¶netmek iÃ§in hem gÃ¶rÃ¼ntÃ¼ Ã¶zelliÄŸini Ã§Ä±karan bir encoder hem de Transformer decoder kullanan bir dil modeli iÃ§eriyor

preprints202504.1582.v1

.

ğŸ§  KullanÄ±lan Mimariler Nelerdir?
1. Transformer Decoder:

Ana ÅŸerit tahmini bu bileÅŸende yapÄ±lÄ±yor.

Her ÅŸerit noktasÄ± (keypoint), bir kelime gibi vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor.

Bu diziler, dil modeli gibi sÄ±rayla iÅŸleniyor.

Causal attention + cross-attention ile Ã§alÄ±ÅŸan bir Transformer decoder var

preprints202504.1582.v1

.

2. Cross-Attention MekanizmasÄ±:

GÃ¶rsel bilgi (resimdeki alanlar) ile dilsel bilgi (ÅŸerit noktalarÄ± dizisi) arasÄ±nda baÄŸ kuruluyor.

Dikkat (attention) hesaplarÄ±nda:

Query (sorgular) â†’ keypoint tokenâ€™lar

Key & Value (anahtar-deÄŸer) â†’ gÃ¶rÃ¼ntÃ¼den Ã§Ä±karÄ±lmÄ±ÅŸ yama (patch) vektÃ¶rleri

Bu yapÄ± tam anlamÄ±yla gÃ¶rsel dil modeli (VLM) mimarisine benziyor.

3. Visual Encoder (CNN + Patch Embedding):

GÃ¶rÃ¼ntÃ¼ Ã¶nce klasik bir CNN ile iÅŸleniyor (ResNet18, DLA34 gibi).

Elde edilen Ã§ok katmanlÄ± Ã¶znitelikler, ViT benzeri yama yama (patch) vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor.

GÃ¶rsel patchâ€™lere pozisyonel kodlamalar da ekleniyor

preprints202504.1582.v1

.

ğŸ§­ Transformer Nerede KullanÄ±lÄ±yor?

Åeritleri bir dizi olarak Ã§Ä±karma sÃ¼reci, tamamen Transformer decoder Ã¼zerinden yÃ¼rÃ¼yor.

GÃ¶rsel Ã¶zelliklerle etkileÅŸim cross-attention ile saÄŸlanÄ±yor (her ÅŸerit noktasÄ±, gÃ¶rselden hangi bÃ¶lgeyle ilgiliyse oraya odaklanÄ±yor).

Modelin ÅŸerit Ã¼retimi autoregressive yani â€œbir sonraki noktayÄ± sÄ±rayla tahmin etmeâ€ temelli.

Yani buradaki Transformer kullanÄ±mÄ±, klasik ViT gibi deÄŸil, dil modeli (language modeling) tarzÄ± bir Transformer.

ğŸ¯ LaneATT gibi Ã§alÄ±ÅŸmalardan farkÄ± ne?
Ã–zellik	LaneATT	LaneLM
Dikkat MekanizmasÄ±	Anchor tabanlÄ± attention	Language-like cross-attention
Åerit Temsili	Koordinat regresyonu (yerel)	Token dizisi olarak (kelime gibi)
Transformer KullanÄ±mÄ±	GÃ¶rsel attention (resim iÃ§inde)	GÃ¶rsel-dil arasÄ±nda Transformer decoder
Girdi FormatÄ±	GÃ¶rÃ¼ntÃ¼	GÃ¶rÃ¼ntÃ¼ + birkaÃ§ nokta (prompt)
Model AmacÄ±	HÄ±zlÄ±, doÄŸrudan ÅŸerit Ã§Ä±karÄ±mÄ±	EtkileÅŸimli, prompt ile yÃ¶nlendirilen
ğŸ” Ã–zetle Ne DÃ¼ÅŸÃ¼nÃ¼yorum?

Bu makale, klasik gÃ¶rsel dikkat yerine dil modeli mimarisiyle lane detection yapan ilk Ã§alÄ±ÅŸmalardan biri. ÅunlarÄ± aÃ§Ä±kÃ§a yapÄ±yor:

GÃ¶rÃ¼ntÃ¼ â†’ yama dizisi (CNN + patch embedding)

Åeritler â†’ token dizisi (x,y koordinatlarÄ± â†’ dil tokenâ€™Ä±)

Transformer decoder ile ÅŸeritleri sÄ±rayla tahmin etme

Dikkat mekanizmasÄ± ile gÃ¶rÃ¼ntÃ¼ ve dil birleÅŸtirme

KullanÄ±cÄ±dan gelen prompt'larla yÃ¶nlendirilebilme (interaktif yapÄ±)

Ancak ÅŸunu belirtmek Ã¶nemli: HenÃ¼z aÃ§Ä±k kaynak kodu yayÄ±nlanmamÄ±ÅŸ, dolayÄ±sÄ±yla â€œkanÄ±tlanmÄ±ÅŸâ€ bir Ã§alÄ±ÅŸma deÄŸil. Kod eriÅŸimi olmadan tekrarlanabilirlik saÄŸlanamÄ±yor â€” bu da bilimsel katkÄ±sÄ±nÄ± sÄ±nÄ±rlÄ±yor.

Ä°stersen bu modeli Ã¶rnek alan basit bir â€œdil modeliyle lane tahminiâ€ deneme kodu fikri Ã§Ä±karabilirim ya da bu mimariyi gerÃ§ek zamanlÄ± bir sistemde nasÄ±l kullanabileceÄŸini konuÅŸabiliriz.

AnladÄ±m! Sana LaneLM tarzÄ± bir mimarinin yapÄ±sÄ±nÄ± sadeleÅŸtirilmiÅŸ, aÃ§Ä±klamalÄ± bir PyTorch kod yapÄ±sÄ± Ã¼zerinden katman katman anlatacaÄŸÄ±m. BÃ¶ylece her bileÅŸenin iÅŸlevini doÄŸrudan kod Ã¼zerinden gÃ¶rÃ¼p anlayabileceksin. AynÄ± zamanda bu mimarinin mantÄ±klÄ± mÄ±, yoksa karmaÅŸÄ±k mÄ± olduÄŸunu da teknik aÃ§Ä±dan deÄŸerlendireceÄŸim.

HazÄ±rlÄ±ÄŸÄ±m tamamlandÄ±ÄŸÄ±nda haber vereceÄŸim, bu sÄ±rada baÅŸka bir ÅŸeyle ilgilenmek istersen buradayÄ±m!


LaneLM Benzeri Åerit Tespit Modeli â€“ Transformer TabanlÄ± Mimari
AÅŸaÄŸÄ±da, LaneLM tarzÄ± bir ÅŸerit tespit modelinin temel bileÅŸenlerini adÄ±m adÄ±m inceliyoruz. Her bir katmanÄ± basit bir PyTorch kod parÃ§acÄ±ÄŸÄ± ile gÃ¶sterip, neden gerekli olduÄŸunu vurgulayacaÄŸÄ±z. Bu mimari, bir CNN tabanlÄ± gÃ¶rsel encoder (Ã¶zellik Ã§Ä±karÄ±cÄ±) ile gÃ¶rÃ¼ntÃ¼den Ã¶zellik haritasÄ± elde eder, bunu Transformer tabanlÄ± bir decoder (kod Ã§Ã¶zÃ¼cÃ¼) ile birleÅŸtirerek oto-regresif (ardÄ±ÅŸÄ±l) bir ÅŸekilde ÅŸerit noktalarÄ±nÄ± Ã§Ä±karÄ±r
preprints.org
preprints.org
. Temel bileÅŸenler: GÃ¶rsel encoder, patch embedding (yama gÃ¶mme), positional encoding (konum kodlamasÄ±), Transformer decoder, cross-attention (Ã§apraz dikkat) ve oto-regresif ÅŸerit Ã§Ä±karÄ±mÄ± olarak sÄ±ralanabilir.
GÃ¶rsel Encoder (CNN Backbone)
GÃ¶rsel encoder, girdi gÃ¶rÃ¼ntÃ¼yÃ¼ alÄ±p daha kompakt bir Ã¶zellik gÃ¶sterimine dÃ¶nÃ¼ÅŸtÃ¼ren CNN tabanlÄ± bir katmandÄ±r. Genellikle ResNet gibi Ã¶nceden eÄŸitilmiÅŸ bir CNN omurgasÄ± kullanÄ±lÄ±r
preprints.org
. AÅŸaÄŸÄ±daki Ã¶rnek kodda basit bir CNN ile Ã¶zellik haritasÄ± Ã§Ä±karÄ±yoruz:
import torch
import torch.nn as nn

class VisualEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        # Basit bir CNN omurgasÄ± (backbone) â€“ birkaÃ§ evriÅŸim katmanÄ±
        self.features = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(32, 64, 3, stride=2, padding=1),
            nn.ReLU(),
            nn.Conv2d(64, 128, 3, stride=2, padding=1),
            nn.ReLU()
        )
    def forward(self, x):
        feature_map = self.features(x)  # Ã‡Ä±ktÄ±: B x 128 x (H/8) x (W/8) boyutlarÄ±nda Ã¶zellik haritasÄ±
        return feature_map
Bu CNN omurgasÄ±, gÃ¶rÃ¼ntÃ¼deki yÃ¼ksek seviyeli Ã¶zellikleri yakalar ve daha kÃ¼Ã§Ã¼k boyutlu bir Ã¶zellik haritasÄ± Ã¼retir. Neden gerekli: Ham piksel girdisini doÄŸrudan bir transformera vermek verimsizdir; bu nedenle CNN, gÃ¶rsel bilgiyi Ã¶zetleyerek transformerin iÅŸlemeyi daha kolay Ã¶ÄŸrenebileceÄŸi bir forma sokar
preprints.org
.
Patch Embedding (Yama GÃ¶mme)
CNN'den gelen 2B Ã¶zellik haritasÄ±nÄ±, Transformer'Ä±n anlayacaÄŸÄ± 1B dizi (sequence) haline getirmek iÃ§in patch embedding yapÄ±lÄ±r. Ã–zellik haritasÄ±, sabit boyutlu parÃ§alara (patch) bÃ¶lÃ¼nÃ¼p her parÃ§a dÃ¼zleÅŸtirilir ve lineer projeksiyonla bir vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r
preprints.org
. Bu, ViT (Vision Transformer) mantÄ±ÄŸÄ±na benzer bir yaklaÅŸÄ±mdÄ±r.
class PatchEmbed(nn.Module):
    def __init__(self, in_channels=128, patch_size=4, embed_dim=256):
        super().__init__()
        self.patch_size = patch_size
        # Ã–zellik haritasÄ±nÄ± parÃ§alara ayÄ±rmak iÃ§in Unfold kullanÄ±yoruz
        self.unfold = nn.Unfold(kernel_size=patch_size, stride=patch_size)
        # Her yama parÃ§asÄ±nÄ± embed_dim boyutunda vektÃ¶re projekte eden lineer katman
        self.proj = nn.Linear(in_channels * patch_size * patch_size, embed_dim)
    def forward(self, feature_map):
        # feature_map: [B, C, H, W]
        patches = self.unfold(feature_map)           # Ã‡Ä±ktÄ±: [B, C*patch_size^2, N_patches]
        patches = patches.transpose(1, 2)            # Åekil deÄŸiÅŸimi: [B, N_patches, C*patch_size^2]
        tokens = self.proj(patches)                 # Her patch'i embed_dim boyutuna eÅŸle
        return tokens  # Boyut: [B, N_patches, embed_dim]
YukarÄ±daki kod, Ã¶zellik haritasÄ±nÄ± patch_size x patch_size boyutlu yamalara bÃ¶ler ve her yamayÄ± bir vektÃ¶r olarak temsil eder. Neden gerekli: Transformer katmanÄ± sabit boyutlu vektÃ¶r dizilerini giriÅŸ olarak alÄ±r; patch embedding, CNNâ€™den gelen 2B veriyi bu gerekli 1B dizi formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r
preprints.org
.
Positional Encoding (Konum KodlamasÄ±)
Patch embedding sonucunda elde edilen token vektÃ¶rleri, uzamsal konum bilgilerini artÄ±k iÃ§ermez Ã§Ã¼nkÃ¼ yamalarÄ± dÃ¼zleÅŸtirdik. Transformerâ€™Ä±n dizideki her tokenâ€™Ä±n gÃ¶rÃ¼ntÃ¼nÃ¼n neresinden geldiÄŸini anlamasÄ± iÃ§in konum bilgisi eklemek gerekir
preprints.org
. Bunu ya sinÃ¼s-kosinÃ¼s fonksiyonlarÄ±yla oluÅŸturulan sabit konum kodlarÄ±yla ya da Ã¶ÄŸrenilebilir bir parametre vektÃ¶rÃ¼yle yapabiliriz.
class PositionalEncoding(nn.Module):
    def __init__(self, embed_dim, max_len=1000):
        super().__init__()
        # Ã–ÄŸrenilebilir konum gÃ¶mme (max_len uzunluÄŸa kadar)
        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, embed_dim))
    def forward(self, tokens):
        # tokens: [B, N, embed_dim]
        seq_len = tokens.size(1)
        # Ä°lk seq_len kadar konum vektÃ¶rÃ¼nÃ¼ ekle
        tokens = tokens + self.pos_embedding[:, :seq_len, :]
        return tokens
Bu katman, her bir gÃ¶rsel token vektÃ¶rÃ¼ne kendi konumuna karÅŸÄ±lÄ±k gelen ek bir vektÃ¶r toplar. Neden gerekli: Konum kodlamasÄ± olmadan Transformer, dizideki yamalarÄ±n sÄ±ralamasÄ±nÄ± ya da uzamsal dÃ¼zenini bilemez; konum bilgisi, her yamanÄ±n gÃ¶rÃ¼ntÃ¼deki yerini modele hissettirir.
Transformer Decoder (Kod Ã‡Ã¶zÃ¼cÃ¼) YapÄ±sÄ±
Bu mimaride, ÅŸerit noktalarÄ±nÄ± ardÄ±ÅŸÄ±l bir dizi olarak tahmin etmek iÃ§in bir Transformer decoder kullanÄ±lÄ±r
preprints.org
. Decoder, dil modellerine benzer ÅŸekilde Ã§alÄ±ÅŸÄ±r: Åu ana kadar Ã¼retilen ÅŸerit noktalarÄ±nÄ± girdide alÄ±p sonraki noktayÄ± tahmin eder. Bunu yaparken hem kendi geÃ§miÅŸ Ã§Ä±ktÄ±larÄ±ndan (self-attention ile) hem de gÃ¶rÃ¼ntÃ¼den elde edilen token dizisinden (cross-attention ile) bilgi alÄ±r. AÅŸaÄŸÄ±da tek bir Transformer decoder katmanÄ±nÄ±n yapÄ±sÄ±nÄ± basitÃ§e gÃ¶steren bir kod yer alÄ±yor:
class DecoderLayer(nn.Module):
    def __init__(self, embed_dim=256, num_heads=8, ff_dim=512):
        super().__init__()
        # 1. Maskeli self-attention: ÅŸerit tokenlarÄ± kendi iÃ§inde dikkat mekanizmasÄ±
        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)
        # 2. Cross-attention: gÃ¶rsel bellek (image tokens) Ã¼zerinde dikkat
        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads)
        # 3. Ä°leri beslemeli aÄŸ (FFN)
        self.ffn = nn.Sequential(
            nn.Linear(embed_dim, ff_dim),
            nn.ReLU(),
            nn.Linear(ff_dim, embed_dim)
        )
        # Katman normlarÄ±
        self.norm1 = nn.LayerNorm(embed_dim)
        self.norm2 = nn.LayerNorm(embed_dim)
        self.norm3 = nn.LayerNorm(embed_dim)
    def forward(self, tgt_seq, memory):
        # tgt_seq: [T, B, E] (ÅŸu ana kadarki ÅŸerit tokenlarÄ±nÄ±n embeddings)
        # memory:  [M, B, E] (gÃ¶rsel tokenlar; CNN+patch emb. sonrasÄ±)
        # Self-Attention (maskeli, kausal)
        attn_out, _ = self.self_attn(tgt_seq, tgt_seq, tgt_seq, 
                                     attn_mask=None)  # gerÃ§ekte kausal maske uygulanÄ±r
        tgt_seq = self.norm1(tgt_seq + attn_out)
        # Cross-Attention (gÃ¶rsel bellek Ã¼zerinden)
        attn_out2, _ = self.cross_attn(tgt_seq, memory, memory)
        tgt_seq = self.norm2(tgt_seq + attn_out2)
        # Feed-forward network
        ff_out = self.ffn(tgt_seq)
        tgt_seq = self.norm3(tgt_seq + ff_out)
        return tgt_seq
YukarÄ±daki DecoderLayer, standart bir Transformer decoder bloÄŸuna benzer ÅŸekilde Ã¶nce self-attention, sonra cross-attention ve ardÄ±ndan bir ileri beslemeli aÄŸ uygular. Self-attention kÄ±smÄ± genellikle gelecek tokenlarÄ± maskeler (kausal mask) ki model kendi henÃ¼z tahmin etmediÄŸi ileriki noktalarÄ± gÃ¶rmesin. Cross-attention kÄ±smÄ± ise bir sonraki noktayÄ± tahmin ederken gÃ¶rÃ¼ntÃ¼den gelen Ã¶zelliklere odaklanmayÄ± saÄŸlar
preprints.org
. Neden gerekli: Decoder, ÅŸerit noktalarÄ±nÄ±n dizisini Ã¼retmek iÃ§in dil modeli mantÄ±ÄŸÄ±nda Ã§alÄ±ÅŸÄ±r; self-attention ile dizinin tutarlÄ±lÄ±ÄŸÄ±nÄ± saÄŸlar, cross-attention ile gÃ¶rsel baÄŸlamdan yararlanÄ±r. Bu sayede model, Ã¶nceki noktalarÄ± ve gÃ¶rÃ¼ntÃ¼ bilgisini bir araya getirerek mantÄ±klÄ± bir sonraki nokta Ã¼retebilir.
Cross-Attention (Ã‡apraz Dikkat MekanizmasÄ±)
Cross-attention, decoder katmanÄ±nÄ±n kritik bir parÃ§asÄ±dÄ±r. Decoderâ€™daki sorgu (query) vektÃ¶rleri ÅŸerit tokenlarÄ±ndan gelirken, anahtar (key) ve deÄŸer (value) vektÃ¶rleri gÃ¶rsel encoder tarafÄ±ndan Ã¼retilen token dizisinden alÄ±nÄ±r
preprints.org
. Bu sayede model, her bir ÅŸerit noktasÄ± tahmininde tÃ¼m gÃ¶rÃ¼ntÃ¼ Ã¶zelliklerine bakabilir. AÅŸaÄŸÄ±daki mini kod parÃ§asÄ± cross-attention kullanÄ±mÄ±nÄ± gÃ¶sterir:
# diyelim ki lane_tokens (T, B, E) ve image_tokens (M, B, E) elimizde var
query = lane_tokens   # sorgu: ÅŸerit tokenlarÄ± (mevcut dizi)
key   = image_tokens  # anahtar: gÃ¶rsel tokenlar
value = image_tokens  # deÄŸer: gÃ¶rsel tokenlar
out, attn_weights = cross_attn(query, key, value)
Burada cross_attn bir MultiheadAttention nesnesidir. Sorgu dizisi, o ana kadarki ÅŸerit noktalarÄ±nÄ± temsil eder; anahtar ve deÄŸer ise gÃ¶rÃ¼ntÃ¼nÃ¼n tÃ¼m patch tokenlarÄ±dÄ±r. SonuÃ§ out, sorgu tokenlarÄ±nÄ±n, gÃ¶rÃ¼ntÃ¼deki hangi bÃ¶lgelere dikkat ettiÄŸini yansÄ±tarak gÃ¼ncellenmiÅŸ temsilidir, attn_weights ise her sorgu tokenÄ±nÄ±n hangi gÃ¶rsel tokenlara ne kadar dikkat verdiÄŸinin aÄŸÄ±rlÄ±klarÄ±dÄ±r. Neden gerekli: Ã‡apraz dikkat, modelin tahmin edeceÄŸi bir sonraki ÅŸerit noktasÄ± iÃ§in gÃ¶rÃ¼ntÃ¼nÃ¼n ilgili bÃ¶lgelerinden bilgi almasÄ±nÄ± saÄŸlar. Bu sayede Ã¼retilen her nokta, gÃ¶rsel konteks ile desteklenmiÅŸ olur
preprints.org
.
Oto-regresif Åerit Ã‡Ä±karÄ±mÄ± (ArdÄ±ÅŸÄ±l Tahmin)
Transformer decoder, ÅŸerit noktalarÄ±nÄ± oto-regresif olarak Ã§Ä±karÄ±r, yani bir seferde bir token (noktayÄ±) Ã¼reterek sÄ±rayla ilerler
preprints.org
. BaÅŸlangÄ±Ã§ta her ÅŸerit iÃ§in bir baÅŸlangÄ±Ã§ bilgisi (Ã¶rn. baÅŸlangÄ±Ã§ noktasÄ± veya Ã¶zel bir <START> tokenÄ±) verilir. Sonra model ardÄ±ÅŸÄ±k olarak her adÄ±mda bir sonraki noktayÄ± tahmin eder ve bu tahmini bir sonraki adÄ±ma giriÅŸ olarak besler. Bu sÃ¼reÃ§, bir <EOS> (dizi sonu) tokenÄ± Ã¼retilene veya maksimum uzunluÄŸa ulaÅŸÄ±lana kadar devam eder
preprints.org
. AÅŸaÄŸÄ±daki kod, bir ÅŸeridin nokta dizisini oto-regresif Ã¼retmeyi basitleÅŸtirerek gÃ¶steriyor:
# VarsayalÄ±m encoder Ã§Ä±ktÄ±larÄ±nÄ± (visual_tokens) elde ettik
visual_tokens = ...  # [M, 1, E] boyutlu gÃ¶rsel bellek (M token, tek resim iÃ§in)
decoder = TransformerDecoder(...)  # birden Ã§ok DecoderLayer iÃ§eren decoder
linear_head = nn.Linear(256, vocab_size)  # tokenlarÄ± id'lere eÅŸleyen Ã§Ä±kÄ±ÅŸ katmanÄ±

lane_sequence = []
input_tokens = [START_TOKEN_ID]  # baÅŸlangÄ±Ã§ tokenÄ±
for step in range(max_len):
    tgt_emb = token_embedding(input_tokens)        # token id'lerini embedding'e Ã§evir
    tgt_emb = pos_encoding(tgt_emb)               # konum bilgisi ekle
    output_emb = decoder(tgt_emb.transpose(0,1),   # shape: [T, B, E] transpoze ile (T adÄ±m, batch=1)
                         visual_tokens)            # gÃ¶rsel bellek ile decode et
    pred_logits = linear_head(output_emb[-1])     # son adÄ±mdaki Ã§Ä±ktÄ± iÃ§in tahminler
    pred_token = torch.argmax(pred_logits, dim=-1).item()  # en olasÄ± tokenÄ± seÃ§
    if pred_token == EOS_TOKEN_ID:
        break  # dizinin sonu
    lane_sequence.append(pred_token)
    input_tokens.append(pred_token)  # yeni tokenÄ± girdiye ekle, dÃ¶ngÃ¼ye devam
YukarÄ±da, TransformerDecoder birden fazla decoder katmanÄ±nÄ± iÃ§eren tÃ¼m kod Ã§Ã¶zÃ¼cÃ¼yÃ¼ temsil ediyor. Her adÄ±mda mevcut ÅŸerit dizi embeddingâ€™ine gÃ¶rsel bellek ile cross-attention uygulayarak bir sonraki tokenÄ± Ã¼retiyoruz. linear_head, decoder Ã§Ä±ktÄ±sÄ±nÄ± gerÃ§ek bir token kimliÄŸine (Ã¶rneÄŸin bir kordinat quantization indeksi) dÃ¶nÃ¼ÅŸtÃ¼ren basit bir doÄŸrusal katmandÄ±r. Ãœretilen token her seferinde dizinin sonuna eklenerek bir sonraki adÄ±ma beslenir. Model, bir <EOS> (End of Sequence) tokenÄ± Ã¼rettiÄŸinde durur
preprints.org
. Neden gerekli: Bu ardÄ±ÅŸÄ±l Ã¼retim yaklaÅŸÄ±mÄ±, ÅŸerit noktalarÄ±nÄ±n deÄŸiÅŸken uzunlukta ve sÄ±ralÄ± bir biÃ§imde Ã§Ä±karÄ±lmasÄ±nÄ± saÄŸlar. Her yeni nokta, Ã¶nceki noktalara ve gÃ¶rÃ¼ntÃ¼ye dayanarak Ã¼retildiÄŸi iÃ§in, ortaya Ã§Ä±kan ÅŸerit geometrik olarak tutarlÄ± olur.
MÃ¼hendis GÃ¶zÃ¼yle DeÄŸerlendirme
Bu LaneLM benzeri mimari, yapÄ±sal olarak mantÄ±klÄ± bir tasarÄ±m sunuyor. CNN tabanlÄ± gÃ¶rsel encoder ile gÃ¼Ã§lÃ¼ gÃ¶rsel Ã¶zellikler Ã§Ä±karÄ±lÄ±rken, Transformer decoder bu Ã¶zellikleri kullanarak ÅŸeritleri bir dizi olarak modelleyebiliyor. Bu sayede model, tÄ±pkÄ± bir cÃ¼mle yazar gibi ÅŸerit noktalarÄ±nÄ± sÄ±rayla Ã¼retebiliyor ve her noktada gÃ¶rÃ¼ntÃ¼nÃ¼n ilgili bÃ¶lÃ¼mlerine bakabiliyor â€“ bu oldukÃ§a yenilikÃ§i ve etkilidir. Mimari birkaÃ§ farklÄ± parÃ§a iÃ§erdiÄŸi iÃ§in karmaÅŸÄ±k gÃ¶rÃ¼nebilir, ancak her bileÅŸenin belirli bir amacÄ± var ve birbirini tamamlÄ±yor. Ã–rneÄŸin, CNNâ€™in Ã§Ä±karamadÄ±ÄŸÄ± dizisel baÄŸÄ±ntÄ±larÄ± Transformer Ã¼stleniyor, Transformerâ€™Ä±n ham veride zorlanacaÄŸÄ± gÃ¶rsel ayrÄ±ntÄ±larÄ± ise CNN saÄŸlÄ±yor. SonuÃ§ olarak, mimari mantÄ±klÄ± bir ÅŸekilde tasarlanmÄ±ÅŸ ve Ã¶zellikle zorlu ÅŸerit tespit senaryolarÄ±nda gÃ¼Ã§lÃ¼ bir performans hedefliyor. Yine de, klasik basit yÃ¶ntemlere kÄ±yasla daha fazla bileÅŸen barÄ±ndÄ±rdÄ±ÄŸÄ± iÃ§in uygulamasÄ± ve eÄŸitimi daha zahmetli olabilir. Ancak elde edilen esneklik ve doÄŸruluk artÄ±ÅŸÄ± gÃ¶z Ã¶nÃ¼ne alÄ±ndÄ±ÄŸÄ±nda, bu ekstra karmaÅŸÄ±klÄ±k gereksiz deÄŸil, amaca hizmet eden bir tasarÄ±m olarak deÄŸerlendirilebilir.
preprints.org
preprints.org
AlÄ±ntÄ±lar

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

LaneLM: Lane Detection as Language Modeling[v1] | Preprints.org

https://www.preprints.org/manuscript/202504.1582

Elbette. LaneLM makalesindeki transformer tabanlÄ± ÅŸerit tespit modelini sÄ±fÄ±rdan uygulayabilmen iÃ§in, benzer aÃ§Ä±k kaynaklÄ± Ã§alÄ±ÅŸmalarÄ± ve bu Ã§alÄ±ÅŸmalarda kullanabileceÄŸin kod kÄ±sÄ±mlarÄ±nÄ± araÅŸtÄ±racaÄŸÄ±m. Ã–zellikle LaneATT, LSTR, LaneFormer, PersFormer gibi projelerde bulunan ilgili transformer, cross-attention veya decoder yapÄ±larÄ±nÄ± Ã¶rnek olarak gÃ¶stereceÄŸim. BÃ¶ylece doÄŸrudan bu projelere bakarak kendi modelini oluÅŸturabileceksin.

AraÅŸtÄ±rmam tamamlandÄ±ÄŸÄ±nda sana hangi projeden hangi yapÄ±larÄ± alabileceÄŸini aÃ§Ä±k ÅŸekilde gÃ¶stereceÄŸim.

Transformer TabanlÄ± Lane Detection Ã–rnekleri
1. LSTR (Lane Shape Prediction with Transformers)
AÃ§Ä±klama: LSTR, bir CNN tabanlÄ± gÃ¶rsel backbone Ã¼zerine transformer bloÄŸu entegre ederek doÄŸrudan ÅŸerit eÄŸrisi parametrelerini Ã§Ä±kartan bir modeldir
ar5iv.labs.arxiv.org
. Transformer katmanÄ±, kendine-Ã¶zgÃ¼ dikkat (self-attention) mekanizmasÄ±yla gÃ¶rÃ¼ntÃ¼deki uzun ve ince ÅŸerit yapÄ±larÄ± ile kÃ¼resel baÄŸlam bilgisini yakalar
ar5iv.labs.arxiv.org
. Model, Ã§Ä±ktÄ±da her bir ÅŸeridi bir polinom parametre vektÃ¶rÃ¼ olarak verir ve bu Ã§Ä±ktÄ±larÄ± Hungarian eÅŸleÅŸtirmesi ile gerÃ§ek veriye atayarak eÄŸitilir. Bu tek-aÅŸamalÄ± yaklaÅŸÄ±m, Ã¶ngÃ¶rÃ¼len ÅŸeritlerle gerÃ§ek ÅŸeritleri bire bir eÅŸleÅŸtirerek NMS (non-max suppression) ihtiyacÄ±nÄ± ortadan kaldÄ±rÄ±r
ar5iv.labs.arxiv.org
.
LaneLMâ€™e FaydasÄ±: LSTRâ€™nin encoder-decoder benzeri yapÄ±sÄ±, Ã¶zellikle transformer decoderâ€™Ä±nÄ±n gÃ¶rÃ¼ntÃ¼ Ã¶zelliklerine cross-attention ile bakarak ÅŸerit parametreleri Ã¼retmesi, LaneLMâ€™i sÄ±fÄ±rdan uygularken yararlÄ± olacaktÄ±r. AyrÄ±ca Hungarian eÅŸleÅŸtirmeli biricik (one-to-one) atama stratejisi ve uÃ§tan uca eÄŸitim mantÄ±ÄŸÄ±, birden fazla ÅŸerit Ã§Ä±ktÄ±sÄ±nÄ± yÃ¶netme konusunda LaneLMâ€™e ilham verebilir.
GitHub: liuruijin17/LSTR (PyTorch uygulamasÄ±, resmi kaynak kodu)
ar5iv.labs.arxiv.org
.
2. O2SFormer (One-to-Several Transformer)
AÃ§Ä±klama: O2SFormer, DETR tarzÄ± (end-to-end transformer tabanlÄ±) bir ÅŸerit tespit mimarisidir. Klasik DETRâ€™nin tek-eÅŸleme (one-to-one) etiket atama kÄ±sÄ±tÄ±nÄ± aÅŸmak iÃ§in one-to-several adÄ± verilen hibrit bir atama stratejisi Ã¶nerir
github.com
. Bu sayede her gerÃ§ek ÅŸerit iÃ§in birden Ã§ok sorgu (query) eÅŸleÅŸmesine izin vererek eÄŸitimi hÄ±zlandÄ±rÄ±r, ancak yine de uÃ§tan uca bir yapÄ±yÄ± korur
github.com
. AyrÄ±ca O2SFormer, transformer decoder aÅŸamasÄ±nda dinamik anchor tabanlÄ± konum sorgularÄ± kullanÄ±r; yani Ã¶nceden tanÄ±mlanmÄ±ÅŸ ÅŸerit anchorâ€™larÄ± Ã¼zerinden konumsal gÃ¶mÃ¼ler oluÅŸturarak sorgulara belirgin uzamsal Ã¶nbilgi katar
github.com
. Katman bazlÄ± yumuÅŸak etiketleme gibi yeniliklerle, O2SFormer CULane gibi veri setlerinde hem transformer tabanlÄ± hem de CNN tabanlÄ± Ã¶nceki yÃ¶ntemleri geÃ§mektedir
github.com
.
LaneLMâ€™e FaydasÄ±: Bu projenin aÃ§Ä±k kaynak kodu, DETR benzeri bir transformer decoderâ€™Ä±n nasÄ±l uygulandÄ±ÄŸÄ±nÄ± gÃ¶sterdiÄŸi iÃ§in deÄŸerlidir. Ã–zellikle cross-attention ile gÃ¶rÃ¼ntÃ¼ Ã¶zelliklerine bakan sorgularÄ±n oluÅŸturulmasÄ±, anchor destekli konumsel gÃ¶mme iÅŸlemleri ve Hungarian benzeri eÅŸleme mantÄ±ÄŸÄ± gibi konular, LaneLM tarzÄ± bir model geliÅŸtirirken doÄŸrudan fayda saÄŸlayacaktÄ±r.
GitHub: zkyseu/O2SFormer (PyTorch uygulamasÄ±, mmdetection tabanlÄ±).
3. CondLSTR (Dynamic Kernels via Transformers)
AÃ§Ä±klama: â€œGenerating Dynamic Kernels via Transformers for Lane Detectionâ€ Ã§alÄ±ÅŸmasÄ± (CondLSTR), transformer yapÄ±sÄ±nÄ± kullanarak her bir ÅŸerit iÃ§in ayrÄ± bir evriÅŸimsel Ã§ekirdek (kernel) Ã¼reten yenilikÃ§i bir yaklaÅŸÄ±mdÄ±r
github.com
. Bir backbone aÄŸÄ±ndan Ã§Ä±kan Ã¶zellik haritasÄ± Ã¼zerinde, transformer decoderâ€™Ä± her ÅŸerit Ã§izgisine karÅŸÄ±lÄ±k gelen dinamik konvolÃ¼syon filtrelerini hesaplar; ardÄ±ndan bu filtreler ilgili ÅŸeridi Ã¶zelleÅŸmiÅŸ olarak tespit etmek iÃ§in gÃ¶rÃ¼ntÃ¼ Ã¶zelliÄŸine uygulanÄ±r
github.com
. Bu sayede model, Ã¶ÄŸrenilmiÅŸ sorgular aracÄ±lÄ±ÄŸÄ±yla her ÅŸeride Ã¶zgÃ¼ bir algÄ±layÄ±cÄ± oluÅŸturmuÅŸ olur.
LaneLMâ€™e FaydasÄ±: CondLSTRâ€™nin kod yapÄ±sÄ±, bir transformer Ã§Ä±ktÄ±sÄ±nÄ±n nasÄ±l downstream bir iÅŸleme (Ã¶r. dinamik evriÅŸim) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lebileceÄŸine dair Ã¶nemli bir Ã¶rnek sunuyor. LaneLM mimarisi kurgulanÄ±rken, bu projedeki cross-attention kullanÄ±mÄ±nÄ±n ÅŸerit bazlÄ± Ã¶zellik Ã§Ä±karma veya ÅŸerit belirteci (token) Ã¼retme iÃ§in nasÄ±l entegre edildiÄŸi incelenebilir. Ã–zellikle gÃ¶rsel Ã¶zellikten gelen tokenâ€™larÄ±n transformer ile iÅŸlenip belirli bir gÃ¶reve (ÅŸerit maskesi oluÅŸturma gibi) yÃ¶nlendirilmesi, LaneLMâ€™de decoder tasarÄ±mÄ± aÃ§Ä±sÄ±ndan yol gÃ¶sterici olacaktÄ±r.
GitHub: czyczyyzc/CondLSTR (PyTorch kodlarÄ±, dinamik konvolÃ¼syon yaklaÅŸÄ±mlÄ±).
4. LATR (Lane detection with TRansformer, 3D)
AÃ§Ä±klama: LATR, tek bir kameradan 3B ÅŸerit tespiti iÃ§in tasarlanmÄ±ÅŸ bir transformer mimarisidir. Bu model, gÃ¶rÃ¼ntÃ¼den kuÅŸbakÄ±ÅŸÄ± dÃ¶nÃ¼ÅŸÃ¼mler oluÅŸturmadan, doÄŸrudan Ã¶n gÃ¶rÃ¼ÅŸ Ã¶zellikleri Ã¼zerinden sorgu-key/value tabanlÄ± cross-attention ile 3B ÅŸeritleri Ã§Ä±karÄ±r
arxiv.org
. LATRâ€™da transformer decoder sorgularÄ±, gÃ¶rÃ¼ntÃ¼den Ã§Ä±karÄ±lan 2B ÅŸerit Ã¶zelliklerine dayalÄ± olarak baÅŸlatÄ±lÄ±r ve her bir sorguya gÃ¼ncellenen bir 3B zemin dÃ¼zlemi Ã¼zerinden hesaplanan dinamik 3B konumsal gÃ¶mme eklenir
arxiv.org
. Bu sayede her sorgu, hem gÃ¶rÃ¼ntÃ¼ iÃ§eriÄŸini hem de uzamsal geometrik bilgiyi bÃ¼nyesinde barÄ±ndÄ±rarak, 3B uzayda ÅŸerit tahmini yapar. LATR, Apollo (sanal) ve OpenLane gibi gerÃ§ek veri setlerinde Ã¶nceki yÃ¶ntemlere kÄ±yasla anlamlÄ± performans artÄ±ÅŸlarÄ± raporlamÄ±ÅŸtÄ±r.
LaneLMâ€™e FaydasÄ±: LATRâ€™Ä±n aÃ§Ä±k kaynak kodu, transformer tabanlÄ± bir ÅŸerit tespit algoritmasÄ±nÄ±n uÃ§tan uca nasÄ±l kurulabileceÄŸini gÃ¶sterir. Ã–zellikle cross-attention mekanizmasÄ± ile sorgularÄ±n gÃ¶rsel Ã¶zellikleri kullanarak nasÄ±l ÅŸerit Ã§Ä±ktÄ±larÄ± Ã¼rettiÄŸi ve sorgularÄ±n baÅŸlangÄ±Ã§ta nasÄ±l tanÄ±mlandÄ±ÄŸÄ± (Ã¶rneÄŸin, gÃ¶rÃ¼ntÃ¼ Ã¶zelliklerinden tÃ¼retilen ÅŸerit ipuÃ§larÄ±yla) gibi konular, LaneLM benzeri bir model geliÅŸtirirken oldukÃ§a deÄŸerlidir. 3B ÅŸerit tespitine odaklansa da, LATRâ€™daki gÃ¶rsel encoder + transformer decoder yapÄ±sÄ± ve kod altyapÄ±sÄ± 2B ÅŸerit modeli iÃ§in bÃ¼yÃ¼k Ã¶lÃ§Ã¼de uyarlanabilir.
GitHub: JMoonr/LATR (PyTorch kodlarÄ±, ICCV 2023 resmi uygulamasÄ±).
AlÄ±ntÄ±lar

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

[2011.04233] End-to-end Lane Shape Prediction with Transformers

https://ar5iv.labs.arxiv.org/html/2011.04233

GitHub - zkyseu/O2SFormer: End-to-End Lane detection with One to Several Transformer

https://github.com/zkyseu/O2SFormer

GitHub - zkyseu/O2SFormer: End-to-End Lane detection with One to Several Transformer

https://github.com/zkyseu/O2SFormer

GitHub - zkyseu/O2SFormer: End-to-End Lane detection with One to Several Transformer

https://github.com/zkyseu/O2SFormer

GitHub - czyczyyzc/CondLSTR: Code for paper "Generating Dynamic Kernels via Transformers for Lane Detection"

https://github.com/czyczyyzc/CondLSTR

[2308.04583] LATR: 3D Lane Detection from Monocular Images with Transformer

https://arxiv.org/abs/2308.04583

