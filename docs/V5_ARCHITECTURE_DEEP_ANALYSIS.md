# LANELM V5 MÄ°MARÄ° - DERÄ°N ANALÄ°Z VE TASARIM PLANI

## ğŸ“‹ Ä°Ã‡Ä°NDEKÄ°LER
1. [Mevcut Mimari (V4) Analizi](#mevcut-mimari-v4-analizi)
2. [KÃ¶k Neden Analizi](#kÃ¶k-neden-analizi)
3. [Yeni Mimari Gereksinimleri](#yeni-mimari-gereksinimleri)
4. [V5 Mimari TasarÄ±mÄ±](#v5-mimari-tasarÄ±mÄ±)
5. [Uygulama PlanÄ±](#uygulama-planÄ±)
6. [Test ve DoÄŸrulama Stratejisi](#test-ve-doÄŸrulama-stratejisi)

---

## 1. MEVCUT MÄ°MARÄ° (V4) ANALÄ°ZÄ°

### 1.1 Mimari BileÅŸenleri

#### **KeypointEmbedding**
```python
# Mevcut: x_tokens + y_tokens + pos_embedding + lane_embedding
keypoint_emb = x_emb + y_emb + pos_emb + lane_emb * 10.0
```
**Sorunlar:**
- X token embedding'i Ã§ok gÃ¼Ã§lÃ¼ (geÃ§miÅŸ X dizisi model'in ana girdisi)
- Y token embedding'i zayÄ±f (zaten sabit 0..T-1)
- Lane embedding signal boosting (x10) var ama yeterli deÄŸil

#### **Decoder Layer (Self-Attention + Cross-Attention)**
```python
# Self-attention: GeÃ§miÅŸ X token'larÄ± arasÄ±nda baÄŸÄ±mlÄ±lÄ±k
attn_out = self_attn(tgt, tgt, tgt, causal_mask)

# Cross-attention: Visual tokens Ã¼zerinde dikkat
attn_out = cross_attn(tgt, memory, memory)
```
**Sorunlar:**
- Self-attention geÃ§miÅŸ X token'lara Ã§ok baÄŸÄ±mlÄ±
- Cross-attention var ama gÃ¶rsel bilgi yeterince gÃ¼Ã§lÃ¼ deÄŸil
- Causal mask geÃ§miÅŸ token'lara odaklanmayÄ± zorunlu kÄ±lÄ±yor

#### **Training vs Inference Mismatch**
```python
# Training (Teacher Forcing):
x_in[:, 0] = x_tokens[:, 0]  # GT'nin ilk deÄŸeri
x_in[:, 1:] = x_tokens[:, :-1]  # GT shifted

# Inference (Autoregressive):
x_in[:, 0] = 0  # Padding token
x_in[:, 1:] = pred_tokens[:, :-1]  # Model'in kendi tahminleri
```
**Sorunlar:**
- Training'de GT ile baÅŸlÄ±yor, inference'da padding ile baÅŸlÄ±yor
- Training'de her adÄ±mda GT gÃ¶rÃ¼yor, inference'da kendi hatalarÄ±nÄ± biriktiriyor
- Exposure bias: Model kendi hatalarÄ±nÄ± gÃ¶rmÃ¼yor

---

## 2. KÃ–K NEDEN ANALÄ°ZÄ°

### 2.1 Temel Sorun: "X Language Model" ParadigmasÄ±

**Mevcut Model MantÄ±ÄŸÄ±:**
```
GeÃ§miÅŸ X Dizisi â†’ Self-Attention â†’ Cross-Attention (Visual) â†’ Gelecek X
```

**Sorun:**
- Model, geÃ§miÅŸ X dizisini **birincil sinyal** olarak kullanÄ±yor
- GÃ¶rsel bilgi **ikincil sinyal** (cross-attention ile ekleniyor)
- GeÃ§miÅŸ X yanlÄ±ÅŸsa, gÃ¶rsel bilgi yeterince gÃ¼Ã§lÃ¼ deÄŸil ki dÃ¼zeltme yapsÄ±n

### 2.2 Cross-Attention ZayÄ±flÄ±ÄŸÄ±

**Mevcut Durum:**
- Cross-attention weights uniform'a yakÄ±n (0.99 uniformity score)
- Model gÃ¶rsel bilgiyi gÃ¶rmezden geliyor
- Visual tokens Ã§ok fazla (P5: 250, Full FPN: ~6500)

**Neden ZayÄ±f:**
1. **Query (tgt) Ã§ok gÃ¼Ã§lÃ¼:** GeÃ§miÅŸ X token'larÄ± zaten yeterli bilgi veriyor
2. **Key/Value (memory) Ã§ok zayÄ±f:** Visual tokens spatial bilgiyi kaybediyor
3. **Attention mekanizmasÄ± yetersiz:** Query'nin gÃ¼cÃ¼ Key/Value'yu eziliyor

### 2.3 Training-Inference Mismatch

**Teacher Forcing:**
- Her adÄ±mda GT X token'Ä± gÃ¶rÃ¼yor
- Model "mÃ¼kemmel geÃ§miÅŸ" ile Ã¶ÄŸreniyor
- Loss dÃ¼ÅŸÃ¼k (0.27) ama gerÃ§ekÃ§i deÄŸil

**Autoregressive:**
- Her adÄ±mda kendi tahminini gÃ¶rÃ¼yor
- Model "hatalÄ± geÃ§miÅŸ" ile Ã§alÄ±ÅŸÄ±yor
- Hatalar birikiyor â†’ zigzagging

**Parallel Decode (Deneme):**
- TÃ¼m adÄ±mlar iÃ§in padding (0) girdisi
- Model gÃ¶rsel bilgiyi kullanamÄ±yor
- Constant prediction (mode collapse)

---

## 3. YENÄ° MÄ°MARÄ° GEREKSÄ°NÄ°MLERÄ°

### 3.1 Temel Prensipler

1. **GÃ¶rsel Bilgi Birincil Sinyal OlmalÄ±**
   - Visual tokens, X token'lardan daha gÃ¼Ã§lÃ¼ olmalÄ±
   - Cross-attention yerine daha direkt bir mekanizma

2. **GeÃ§miÅŸ X BaÄŸÄ±mlÄ±lÄ±ÄŸÄ± AzaltÄ±lmalÄ±**
   - Self-attention zayÄ±flatÄ±lmalÄ± veya kaldÄ±rÄ±lmalÄ±
   - X token embedding'i azaltÄ±lmalÄ±

3. **Training-Inference Uyumu**
   - Training ve inference aynÄ± rejimde Ã§alÄ±ÅŸmalÄ±
   - Exposure bias ortadan kaldÄ±rÄ±lmalÄ±

4. **Y KoordinatÄ± Sabit KalmalÄ±**
   - Y token'larÄ± zaten sabit (0..T-1)
   - Y-loss gereksiz, sadece X-loss yeterli

### 3.2 Mimari DeÄŸiÅŸiklik Stratejileri

#### **Strateji A: Visual-First Decoder**
- Cross-attention'Ä± gÃ¼Ã§lendir
- Self-attention'Ä± zayÄ±flat veya kaldÄ±r
- Visual tokens'Ä± daha gÃ¼Ã§lÃ¼ encode et

#### **Strateji B: Non-Autoregressive Decoder**
- TÃ¼m X token'larÄ±nÄ± paralel tahmin et
- GeÃ§miÅŸ X baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± tamamen kaldÄ±r
- Sadece gÃ¶rsel bilgi + Y grid kullan

#### **Strateji C: Hybrid Approach**
- Ä°lk birkaÃ§ token iÃ§in visual-first
- Sonraki token'lar iÃ§in autoregressive (ama zayÄ±f)

---

## 4. V5 MÄ°MARÄ° TASARIMI

### 4.1 SeÃ§ilen Strateji: **Visual-First Decoder (Strateji A)**

**Neden:**
- En az invazif (mevcut kodu minimal deÄŸiÅŸtirerek)
- En hÄ±zlÄ± implement edilebilir
- En az risk (mevcut baÅŸarÄ±larÄ± koruyarak)

### 4.2 Mimari DeÄŸiÅŸiklikleri

#### **4.2.1 Visual Token Encoder GÃ¼Ã§lendirme**

**Mevcut:**
```python
# P5 Only: (B, 64, 10, 25) -> 250 tokens
# Full FPN: (B, 64, 20, 50) + (B, 64, 10, 25) + (B, 64, 5, 13) -> ~6500 tokens
```

**Yeni:**
```python
# P5 Only + Spatial Pooling:
# (B, 64, 10, 25) -> Adaptive Pooling -> (B, 64, 5, 13) -> 65 tokens
# Daha az token, daha gÃ¼Ã§lÃ¼ spatial bilgi
```

**DeÄŸiÅŸiklikler:**
1. **Adaptive Spatial Pooling:** Visual tokens sayÄ±sÄ±nÄ± azalt (250 -> 65)
2. **Stronger Positional Encoding:** 2D PE'yi gÃ¼Ã§lendir
3. **Feature Normalization:** LayerNorm + Feature Scaling

#### **4.2.2 Keypoint Embedding ZayÄ±flatma**

**Mevcut:**
```python
keypoint_emb = x_emb + y_emb + pos_emb + lane_emb * 10.0
```

**Yeni:**
```python
# X embedding'i zayÄ±flat (geÃ§miÅŸ X'e daha az baÄŸÄ±mlÄ±lÄ±k)
x_emb_scaled = x_emb * 0.3  # 1.0 -> 0.3
# Y ve pos embedding'i koru
# Lane embedding'i gÃ¼Ã§lendir (gÃ¶rsel bilgi ile birlikte)
lane_emb_scaled = lane_emb * 15.0  # 10.0 -> 15.0
keypoint_emb = x_emb_scaled + y_emb + pos_emb + lane_emb_scaled
```

**DeÄŸiÅŸiklikler:**
1. **X Embedding Scaling:** 1.0 -> 0.3 (geÃ§miÅŸ X'e daha az baÄŸÄ±mlÄ±lÄ±k)
2. **Lane Embedding Boost:** 10.0 -> 15.0 (hangi lane'i tahmin ettiÄŸini vurgula)

#### **4.2.3 Decoder Layer Yeniden TasarÄ±mÄ±**

**Mevcut:**
```python
# 1. Self-attention (causal, gÃ¼Ã§lÃ¼)
# 2. Cross-attention (visual, zayÄ±f)
# 3. FFN
```

**Yeni:**
```python
# 1. Cross-attention FIRST (visual, gÃ¼Ã§lÃ¼)
# 2. Self-attention SECOND (causal, zayÄ±f)
# 3. FFN
# 4. Visual-Query Fusion (yeni)
```

**DeÄŸiÅŸiklikler:**
1. **SÄ±ra DeÄŸiÅŸikliÄŸi:** Cross-attention Ã¶nce, self-attention sonra
2. **Self-Attention ZayÄ±flatma:** Dropout artÄ±r (0.0 -> 0.2)
3. **Cross-Attention GÃ¼Ã§lendirme:** Multi-head sayÄ±sÄ±nÄ± artÄ±r (8 -> 16)
4. **Visual-Query Fusion:** Cross-attention output'unu query'ye ekle (residual connection)

#### **4.2.4 Training Stratejisi**

**Mevcut:**
```python
# Teacher Forcing: x_in = GT shifted
# Scheduled Sampling: %20 oranÄ±nda model tahmini kullan
```

**Yeni:**
```python
# Visual-First Training:
# 1. Ä°lk epoch'larda: Pure Teacher Forcing (stabilite)
# 2. Sonraki epoch'larda: Scheduled Sampling (%30-50)
# 3. Son epoch'larda: AR Rollout Loss (kÄ±sa sequence, 5-10 step)
```

**DeÄŸiÅŸiklikler:**
1. **Scheduled Sampling ArtÄ±ÅŸÄ±:** %20 -> %30-50
2. **AR Rollout Loss Ekleme:** 5-10 step autoregressive loss ekle
3. **Progressive Training:** AÅŸamalÄ± olarak exposure bias'Ä± azalt

---

## 5. UYGULAMA PLANI

### 5.1 Faz 1: Visual Token Encoder GÃ¼Ã§lendirme

**AdÄ±mlar:**
1. `VisualTokenEncoder`'a adaptive pooling ekle
2. Token sayÄ±sÄ±nÄ± azalt (250 -> 65)
3. 2D PE'yi gÃ¼Ã§lendir
4. Test: Token sayÄ±sÄ± ve spatial bilgi korunuyor mu?

**Dosyalar:**
- `libs/models/lanelm/model.py`: `VisualTokenEncoder` class

**Beklenen SonuÃ§:**
- Visual tokens sayÄ±sÄ± azalÄ±r
- Spatial bilgi korunur
- Cross-attention daha etkili olur

### 5.2 Faz 2: Keypoint Embedding ZayÄ±flatma

**AdÄ±mlar:**
1. `KeypointEmbedding`'e scaling parametreleri ekle
2. X embedding'i 0.3'e scale et
3. Lane embedding'i 15.0'a boost et
4. Test: GeÃ§miÅŸ X baÄŸÄ±mlÄ±lÄ±ÄŸÄ± azalÄ±yor mu?

**Dosyalar:**
- `libs/models/lanelm/model.py`: `KeypointEmbedding` class

**Beklenen SonuÃ§:**
- GeÃ§miÅŸ X token'lara baÄŸÄ±mlÄ±lÄ±k azalÄ±r
- GÃ¶rsel bilgi daha Ã¶nemli hale gelir
- Training loss biraz artabilir (normal)

### 5.3 Faz 3: Decoder Layer Yeniden TasarÄ±mÄ±

**AdÄ±mlar:**
1. `LaneLMDecoderLayer`'da sÄ±ra deÄŸiÅŸikliÄŸi (cross-attention Ã¶nce)
2. Self-attention dropout artÄ±r (0.0 -> 0.2)
3. Cross-attention head sayÄ±sÄ±nÄ± artÄ±r (8 -> 16)
4. Visual-Query Fusion ekle
5. Test: Cross-attention weights daha non-uniform mu?

**Dosyalar:**
- `libs/models/lanelm/model.py`: `LaneLMDecoderLayer` class

**Beklenen SonuÃ§:**
- Cross-attention weights non-uniform olur
- GÃ¶rsel bilgi daha etkili kullanÄ±lÄ±r
- Self-attention zayÄ±flar

### 5.4 Faz 4: Training Stratejisi GÃ¼ncelleme

**AdÄ±mlar:**
1. Scheduled Sampling oranÄ±nÄ± artÄ±r (%20 -> %30-50)
2. AR Rollout Loss ekle (5-10 step)
3. Progressive training schedule ekle
4. Test: Autoregressive inference hatasÄ± azalÄ±yor mu?

**Dosyalar:**
- `tools/train_lanelm_v4_fixed.py`: Training loop

**Beklenen SonuÃ§:**
- Training ve inference rejimleri yakÄ±nlaÅŸÄ±r
- Exposure bias azalÄ±r
- Autoregressive inference hatasÄ± azalÄ±r

### 5.5 Faz 5: Inference Optimizasyonu

**AdÄ±mlar:**
1. Inference'da visual-first decode kullan
2. GeÃ§miÅŸ X token'lara daha az baÄŸÄ±mlÄ±lÄ±k
3. Smoothing gÃ¼Ã§lendir (window_length=15 -> 21)
4. Test: Zigzagging azalÄ±yor mu?

**Dosyalar:**
- `libs/models/detectors/lanelm_detector.py`: `autoregressive_decode`
- `tools/train_lanelm_v4_fixed.py`: `visualize` function

**Beklenen SonuÃ§:**
- Zigzagging azalÄ±r
- GÃ¶rsel kalite artar
- Inference hÄ±zÄ± korunur

---

## 6. TEST VE DOÄRULAMA STRATEJÄ°SÄ°

### 6.1 Her Faz Ä°Ã§in Test

**Faz 1 Test:**
- Visual token sayÄ±sÄ± kontrolÃ¼
- Spatial bilgi korunuyor mu?
- Cross-attention uniformity score

**Faz 2 Test:**
- GeÃ§miÅŸ X baÄŸÄ±mlÄ±lÄ±ÄŸÄ± Ã¶lÃ§Ã¼mÃ¼
- Training loss deÄŸiÅŸimi
- Visual attention aÄŸÄ±rlÄ±klarÄ±

**Faz 3 Test:**
- Cross-attention weights non-uniform mu?
- Self-attention zayÄ±fladÄ± mÄ±?
- Training loss stabil mi?

**Faz 4 Test:**
- Scheduled Sampling etkisi
- AR Rollout Loss etkisi
- Autoregressive inference hatasÄ±

**Faz 5 Test:**
- Zigzagging azaldÄ± mÄ±?
- GÃ¶rsel kalite arttÄ± mÄ±?
- Inference hÄ±zÄ± korundu mu?

### 6.2 Genel Test SenaryolarÄ±

**1-Image Overfit:**
- Loss < 0.3 olmalÄ±
- Teacher Forcing: mean_err < 1px
- Autoregressive: mean_err < 20px (Ã¶nceden 38px)

**8-Image Overfit:**
- Loss < 0.5 olmalÄ±
- GÃ¶rsel kalite iyi olmalÄ±
- Zigzagging minimal olmalÄ±

**100-Image Training:**
- Loss < 0.3 olmalÄ±
- GÃ¶rsel kalite iyi olmalÄ±
- Zigzagging minimal olmalÄ±

**Full Dataset Training:**
- Loss < 0.5 olmalÄ±
- CULane F1 > 0.5 olmalÄ± (Ã¶nceden 0.0)

---

## 7. RÄ°SK ANALÄ°ZÄ° VE YEDEK PLANLAR

### 7.1 Riskler

**Risk 1: Visual Token SayÄ±sÄ± Azaltma**
- **Risk:** Spatial bilgi kaybÄ±
- **Yedek:** Adaptive pooling yerine learnable pooling kullan

**Risk 2: X Embedding ZayÄ±flatma**
- **Risk:** Model hiÃ§ Ã¶ÄŸrenemez
- **Yedek:** Scaling'i daha yumuÅŸak yap (0.3 -> 0.5)

**Risk 3: Decoder SÄ±ra DeÄŸiÅŸikliÄŸi**
- **Risk:** Training instabil olur
- **Yedek:** SÄ±ra deÄŸiÅŸikliÄŸi yerine fusion kullan

**Risk 4: Scheduled Sampling ArtÄ±ÅŸÄ±**
- **Risk:** Training yavaÅŸlar
- **Yedek:** Progressive schedule kullan

### 7.2 Yedek Planlar

**Plan B: Non-Autoregressive Decoder**
- EÄŸer Strateji A baÅŸarÄ±sÄ±z olursa
- TÃ¼m X token'larÄ±nÄ± paralel tahmin et
- Daha radikal ama daha etkili olabilir

**Plan C: Hybrid Approach**
- Ä°lk 5 token visual-first
- Sonraki token'lar autoregressive
- En dengeli yaklaÅŸÄ±m

---

## 8. BAÅARI KRÄ°TERLERÄ°

### 8.1 Minimum BaÅŸarÄ± Kriterleri

1. **1-Image Overfit:**
   - Loss < 0.3 âœ…
   - Teacher Forcing: mean_err < 1px âœ…
   - Autoregressive: mean_err < 25px (Ã¶nceden 38px) âœ…

2. **8-Image Overfit:**
   - Loss < 0.5 âœ…
   - GÃ¶rsel kalite iyi âœ…
   - Zigzagging minimal âœ…

3. **100-Image Training:**
   - Loss < 0.3 âœ…
   - GÃ¶rsel kalite iyi âœ…
   - Zigzagging minimal âœ…

### 8.2 Ä°deal BaÅŸarÄ± Kriterleri

1. **Autoregressive Inference:**
   - mean_err < 15px (Ã¶nceden 38px)
   - Zigzagging yok
   - GÃ¶rsel kalite mÃ¼kemmel

2. **Full Dataset Training:**
   - Loss < 0.4
   - CULane F1 > 0.6 (Ã¶nceden 0.0)
   - GÃ¶rsel kalite mÃ¼kemmel

---

## 9. SONUÃ‡ VE SONRAKÄ° ADIMLAR

### 9.1 Ã–zet

**Mevcut Sorun:**
- Model "X language model" gibi Ã§alÄ±ÅŸÄ±yor
- GeÃ§miÅŸ X token'lara Ã§ok baÄŸÄ±mlÄ±
- GÃ¶rsel bilgi yeterince gÃ¼Ã§lÃ¼ deÄŸil
- Training-inference mismatch

**Ã‡Ã¶zÃ¼m:**
- Visual-First Decoder yaklaÅŸÄ±mÄ±
- GeÃ§miÅŸ X baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± azalt
- GÃ¶rsel bilgiyi gÃ¼Ã§lendir
- Training-inference uyumunu artÄ±r

### 9.2 Sonraki AdÄ±mlar

1. **Faz 1'i uygula:** Visual Token Encoder gÃ¼Ã§lendirme
2. **Test et:** 1-image overfit ile doÄŸrula
3. **Faz 2'yi uygula:** Keypoint Embedding zayÄ±flatma
4. **Test et:** GeÃ§miÅŸ X baÄŸÄ±mlÄ±lÄ±ÄŸÄ± azaldÄ± mÄ±?
5. **Faz 3'Ã¼ uygula:** Decoder Layer yeniden tasarÄ±mÄ±
6. **Test et:** Cross-attention weights non-uniform mu?
7. **Faz 4'Ã¼ uygula:** Training stratejisi gÃ¼ncelleme
8. **Test et:** Autoregressive inference hatasÄ± azaldÄ± mÄ±?
9. **Faz 5'i uygula:** Inference optimizasyonu
10. **Test et:** Zigzagging azaldÄ± mÄ±?

---

**Tarih:** 2024-12-30
**Versiyon:** 1.0
**Durum:** Analiz TamamlandÄ±, Uygulama Bekliyor

