# KRÄ°TÄ°K SORUN: LOSS VS. GÃ–RSEL KALÄ°TE UYUMSUZLUÄU

## ğŸ“Š DURUM

**Loss:**
- X-loss: 0.1359 âœ… (MÃ¼kemmel!)
- Total loss: 0.2698 âœ… (Ã‡ok dÃ¼ÅŸÃ¼k!)

**GÃ¶rsel Kalite:**
- GÃ¶rÃ¼ntÃ¼lerde hala zigzag var âŒ
- Loss dÃ¼ÅŸÃ¼k ama gÃ¶rsel kalite kÃ¶tÃ¼ âŒ

---

## ğŸ” SORUN ANALÄ°ZÄ°

### 1. **TOKEN-LEVEL LOSS vs. PIXEL-LEVEL KALÄ°TE**

**Sorun:**
- Cross-entropy loss token-level (doÄŸru token tahmin et)
- Ama gÃ¶rsel kalite pixel-level (dÃ¼zgÃ¼n Ã§izgi)
- Model token'larÄ± doÄŸru Ã¶ÄŸreniyor ama decode ederken zigzag oluyor

**Ã–rnek:**
- Model token 100, 101, 102, 103 tahmin ediyor (doÄŸru)
- Ama decode edilince: X=400, 404, 398, 405 (zigzag!)

---

### 2. **Y-LOSS YÃœKSEK**

**Sorun:**
- Y-loss: 2.8147 (Ã§ok yÃ¼ksek!)
- X-loss: 0.1359 (Ã§ok dÃ¼ÅŸÃ¼k!)
- Y-loss >> X-loss â†’ Model Y koordinatlarÄ±nÄ± Ã¶ÄŸrenemiyor

**Neden:**
- Y token'larÄ± zaten sÄ±ralÄ± (0,1,2,...,39)
- Ama model bunlarÄ± Ã¶ÄŸrenmekte zorlanÄ±yor
- Y-loss gereksiz olabilir

---

### 3. **SMOOTHING YETERSÄ°Z**

**Mevcut:**
- `window_length=11` (savgol_filter)
- `smooth=True` kullanÄ±lÄ±yor
- Ama hala zigzag var

**Sorun:**
- Smoothing decode sonrasÄ± uygulanÄ±yor
- Ama token'lar zaten zigzag ise smoothing yeterli deÄŸil

---

### 4. **VISUALIZATION DECODE FARKLI**

**Sorun:**
- Training'de: Teacher forcing (GT input)
- Visualization'da: Autoregressive decode (model'in kendi output'u)
- Bu farklÄ±lÄ±k sorun yaratabilir

---

## âœ… Ã‡Ã–ZÃœM Ã–NERÄ°LERÄ°

### **Ã‡Ã–ZÃœM 1: Y-LOSS'U TAMAMEN KALDIR** â­ EN Ã–NEMLÄ°

**MantÄ±k:**
- Y token'larÄ± zaten sÄ±ralÄ± (0,1,2,...,39)
- Model bunlarÄ± zaten biliyor
- Y-loss gereksiz ve zararlÄ± (2.81 loss ekliyor)

**Test:**
- Sadece X-loss ile devam et
- Y token'larÄ±nÄ± sabit tut (0,1,2,...,39)
- GÃ¶rÃ¼ntÃ¼lerde zigzag azalÄ±yor mu kontrol et

---

### **Ã‡Ã–ZÃœM 2: SMOOTHING GÃœÃ‡LENDÄ°R**

**Mevcut:**
```python
window_length = min(11, len(x_eval))
```

**Ã–neri:**
```python
window_length = min(15, len(x_eval))  # Daha gÃ¼Ã§lÃ¼ smoothing
# VEYA
window_length = min(21, len(x_eval))  # Ã‡ok gÃ¼Ã§lÃ¼ smoothing
```

---

### **Ã‡Ã–ZÃœM 3: PIXEL-LEVEL LOSS EKLE**

**MantÄ±k:**
- Token loss + Pixel loss
- Decode edilmiÅŸ koordinatlarÄ±n GT ile karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±

**Kod:**
```python
# Decode predictions
pred_coords = tokenizer.decode_single_lane(x_tokens, y_tokens, smooth=False)
gt_coords = tokenizer.decode_single_lane(x_tokens_gt, y_tokens_gt, smooth=False)

# Pixel-level loss (L1 or L2)
pixel_loss = F.l1_loss(pred_coords, gt_coords)

# Combined loss
loss = 0.9 * token_loss + 0.1 * pixel_loss
```

---

### **Ã‡Ã–ZÃœM 4: VISUALIZATION DECODE Ä°YÄ°LEÅTÄ°R**

**Sorun:**
- Visualization'da autoregressive decode farklÄ±
- Training'deki gibi yap

**Ã–neri:**
- Training'deki decode logic'i kullan
- VEYA inference-time smoothing ekle

---

## ğŸ¯ Ã–NCELÄ°K SIRASI

1. **Y-LOSS'U TAMAMEN KALDIR** â† EN Ã–NEMLÄ°
2. **SMOOTHING GÃœÃ‡LENDÄ°R** â† HIZLI Ã‡Ã–ZÃœM
3. **PIXEL-LEVEL LOSS EKLE** â† UZUN VADELÄ°
4. **VISUALIZATION DECODE Ä°YÄ°LEÅTÄ°R** â† DEBUG Ä°Ã‡Ä°N

---

## ğŸ“Š BEKLENEN SONUÃ‡LAR

**Y-loss kaldÄ±rÄ±lÄ±rsa:**
- Total loss = X-loss (0.14) âœ…
- Y-loss sorunu ortadan kalkar
- GÃ¶rÃ¼ntÃ¼lerde zigzag azalÄ±r (Y koordinatlarÄ± sabit)

**Smoothing gÃ¼Ã§lendirilirse:**
- Zigzag azalÄ±r
- Ama root cause Ã§Ã¶zÃ¼lmez (token'lar hala zigzag)

**Pixel-level loss eklenirse:**
- Model dÃ¼zgÃ¼n Ã§izgiler Ã¶ÄŸrenir
- Ama training yavaÅŸlar

