# KRÄ°TÄ°K BUG'LAR - DETAYLI ANALÄ°Z

## ğŸ” BULUNAN SORUNLAR

### **BUG 1: BAÅLANGIÃ‡ TOKEN MÄ°SMATCH** â­ EN KRÄ°TÄ°K

**Training (line 359):**
```python
x_in[:, 0] = x_tokens[:, 0]  # GT'nin ilk deÄŸeri
```

**Visualization (line 137-144):**
```python
for t in range(T):
    x_in = x_out.clone()
    if t > 0:
        x_in_shifted = torch.zeros_like(x_in)
        x_in_shifted[:, 1:t+1] = x_out[:, :t]
    else:
        x_in_shifted = x_in  # x_in[:, 0] = 0 (PADDING!)
```

**Sorun:**
- Training'de model GT'nin ilk deÄŸeri ile baÅŸlÄ±yor
- Visualization'da model padding token (0) ile baÅŸlÄ±yor
- Model training'de "GT ile baÅŸla" Ã¶ÄŸreniyor, visualize'da "padding ile baÅŸla" gÃ¶rÃ¼yor
- Bu mismatch zigzag'a neden oluyor!

---

### **BUG 2: AUTOREGRESSIVE DECODE YANLIÅ**

**Visualization (line 137-150):**
```python
for t in range(T):
    x_in = x_out.clone()  # x_out baÅŸlangÄ±Ã§ta tÃ¼mÃ¼ 0
    if t > 0:
        x_in_shifted = torch.zeros_like(x_in)
        x_in_shifted[:, 1:t+1] = x_out[:, :t]  # Ã–nceki token'larÄ± kopyala
        # x_in_shifted[:, 0] = 0 kalÄ±yor! âŒ
    else:
        x_in_shifted = x_in  # t=0'da x_in[:, 0] = 0 âŒ
    
    logits_x, _ = model(visual_tokens[:1], x_in_shifted, y_fixed, lane_indices=lane_ids)
    pred_x = torch.argmax(logits_x[0, t], dim=-1)
    x_out[0, t] = pred_x
```

**Sorun:**
- Her t'de `x_in_shifted[:, 0] = 0` kalÄ±yor
- Model her t'de "baÅŸlangÄ±Ã§ token'Ä± 0" gÃ¶rÃ¼yor
- Ama training'de `x_in[:, 0] = x_tokens[:, 0]` (GT'nin ilk deÄŸeri)
- Bu mismatch model'i ÅŸaÅŸÄ±rtÄ±yor!

---

### **BUG 3: Y TOKEN FÄ°LTERÄ°NG EKSÄ°K**

**Visualization (line 163-167):**
```python
valid_mask = x_tokens > 0  # Sadece X token'larÄ± filtreleniyor
x_filtered = x_tokens[valid_mask]
y_filtered = y_tokens[valid_mask]  # Y token'larÄ± da filtreleniyor ama...
```

**Sorun:**
- `y_fixed = torch.arange(T)` (0,1,2,...,T-1) kullanÄ±lÄ±yor
- Bu her zaman geÃ§erli (padding yok)
- Ama GT'de bazÄ± Y token'larÄ± padding olabilir (y_tok >= T)
- Decode edilirken `y_tok >= T` kontrolÃ¼ yapÄ±lÄ±yor (line 225)
- Ama visualization'da bu kontrol yapÄ±lmÄ±yor!

---

### **BUG 4: DECODE SORUNU**

**Tokenizer (line 225):**
```python
if x_tok == self.cfg.pad_token_x or y_tok >= self.T:
    continue  # Padding token'larÄ± atla
```

**Sorun:**
- Visualization'da `y_fixed = torch.arange(T)` kullanÄ±lÄ±yor
- Bu her zaman geÃ§erli (y_tok < T)
- Ama GT'de bazÄ± Y token'larÄ± padding olabilir
- Decode edilirken bu kontrol yapÄ±lÄ±yor ama visualization'da y_fixed kullanÄ±ldÄ±ÄŸÄ± iÃ§in sorun yok
- Ama yine de yanlÄ±ÅŸ!

---

## âœ… Ã‡Ã–ZÃœMLER

### **FIX 1: BAÅLANGIÃ‡ TOKEN DÃœZELT**

**Ã–nceki:**
```python
x_in_shifted = x_in  # t=0'da x_in[:, 0] = 0
```

**Yeni:**
```python
# Training'deki gibi: GT'nin ilk deÄŸerini kullan
# Ama GT yok, o yÃ¼zden model'in kendi tahminini kullan
# VEYA ilk token'Ä± Ã¶zel olarak tahmin et
if t == 0:
    # Ä°lk token iÃ§in Ã¶zel iÅŸlem
    # Model'e boÅŸ sequence ver, ilk token'Ä± tahmin et
    x_in_first = torch.zeros(1, T, dtype=torch.long, device=device)
    logits_x_first, _ = model(visual_tokens[:1], x_in_first, y_fixed, lane_indices=lane_ids)
    pred_x_first = torch.argmax(logits_x_first[0, 0], dim=-1)
    x_out[0, 0] = pred_x_first
    continue  # Ä°lk token'Ä± atla, sonraki token'lara geÃ§
```

**VEYA:**
```python
# Daha basit: Training'deki gibi baÅŸlangÄ±Ã§ token'Ä±nÄ± kullan
# Ama GT yok, o yÃ¼zden model'in kendi tahminini kullan
if t == 0:
    x_in_shifted = torch.zeros(1, T, dtype=torch.long, device=device)
    # Ä°lk token iÃ§in Ã¶zel tahmin
    logits_x, _ = model(visual_tokens[:1], x_in_shifted, y_fixed, lane_indices=lane_ids)
    pred_x = torch.argmax(logits_x[0, 0], dim=-1)
    x_out[0, 0] = pred_x
else:
    # Sonraki token'lar iÃ§in normal autoregressive decode
    x_in_shifted = torch.zeros_like(x_in)
    x_in_shifted[:, 1:t+1] = x_out[:, :t]
    # x_in_shifted[:, 0] = x_out[:, 0]  # Ä°lk token'Ä± koru!
    logits_x, _ = model(visual_tokens[:1], x_in_shifted, y_fixed, lane_indices=lane_ids)
    pred_x = torch.argmax(logits_x[0, t], dim=-1)
    x_out[0, t] = pred_x
```

---

### **FIX 2: AUTOREGRESSIVE DECODE DÃœZELT**

**Ã–nceki:**
```python
x_in_shifted[:, 1:t+1] = x_out[:, :t]
# x_in_shifted[:, 0] = 0 kalÄ±yor!
```

**Yeni:**
```python
x_in_shifted[:, 0] = x_out[:, 0]  # Ä°lk token'Ä± koru!
x_in_shifted[:, 1:t+1] = x_out[:, :t]
```

---

### **FIX 3: Y TOKEN FÄ°LTERÄ°NG EKLE**

**Ã–nceki:**
```python
valid_mask = x_tokens > 0  # Sadece X token'larÄ±
```

**Yeni:**
```python
valid_mask = (x_tokens > 0) & (y_tokens < T)  # X ve Y token'larÄ±
```

---

## ğŸ¯ Ã–NCELÄ°K SIRASI

1. **FIX 1: BAÅLANGIÃ‡ TOKEN DÃœZELT** â† EN KRÄ°TÄ°K
2. **FIX 2: AUTOREGRESSIVE DECODE DÃœZELT** â† Ã–NEMLÄ°
3. **FIX 3: Y TOKEN FÄ°LTERÄ°NG EKLE** â† Ä°YÄ°LEÅTÄ°RME

